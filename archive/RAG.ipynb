{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm=\"llama3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "\n",
    "def initialize_vector_store():\n",
    "    # Step 1: Read the contents of documents.txt\n",
    "    with open('db/documents.txt', 'r') as file:\n",
    "        documents = file.readlines()\n",
    "\n",
    "    # Ensure the documents are in the right format (list of strings)\n",
    "    documents = [doc.strip() for doc in documents]\n",
    "\n",
    "    # Step 2: Initialize the embedding model\n",
    "    embedding = FastEmbedEmbeddings()\n",
    "\n",
    "    # Step 3: Initialize the Chroma vector store with the embedding function\n",
    "    persist_directory = \"db\"\n",
    "    if not os.path.exists(persist_directory):\n",
    "        os.makedirs(persist_directory)\n",
    "\n",
    "    vector_store = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "\n",
    "    # Step 4: Add documents to the vector store\n",
    "    for document in documents:\n",
    "        vector_store.add_texts([document])\n",
    "\n",
    "    return vector_store\n",
    "\n",
    "def get_retriever():\n",
    "    persist_directory = \"db\"\n",
    "    \n",
    "    # Check if vector store already exists\n",
    "    if not os.path.exists(persist_directory):\n",
    "        vector_store = initialize_vector_store()\n",
    "    else:\n",
    "        # Initialize Chroma vector store without re-adding texts\n",
    "        embedding = FastEmbedEmbeddings()\n",
    "        vector_store = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "\n",
    "    # Step 5: Create the retriever\n",
    "    retriever = vector_store.as_retriever()\n",
    "    return retriever\n",
    "\n",
    "# Example usage\n",
    "retriever = get_retriever()\n",
    "print(\"Retriever created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the retriever\n",
    "query = \"Your query here\"\n",
    "results = retriever(query)\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def parse_and_save_excel(file_path, sheet_name, save_path):\n",
    "    \"\"\"\n",
    "    Parse a specific sheet in the Excel file and save the DataFrame to a file.\n",
    "    \n",
    "    :param file_path: Path to the Excel file.\n",
    "    :param sheet_name: Name or index of the sheet to parse.\n",
    "    :param save_path: Path to save the DataFrame.\n",
    "    \"\"\"\n",
    "    # Load the specific sheet into a DataFrame\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "    \n",
    "    # Save the DataFrame to a file\n",
    "    with open(save_path, 'wb') as file:\n",
    "        pickle.dump(df, file)\n",
    "    print(f\"DataFrame from sheet '{sheet_name}' saved to {save_path}\")\n",
    "\n",
    "def load_df(save_path):\n",
    "    \"\"\"\n",
    "    Load the saved DataFrame from a file.\n",
    "    \n",
    "    :param save_path: Path to the saved DataFrame.\n",
    "    :return: Loaded DataFrame.\n",
    "    \"\"\"\n",
    "    with open(save_path, 'rb') as file:\n",
    "        df = pickle.load(file)\n",
    "    return df\n",
    "\n",
    "def search_df(df, search_column, search_value):\n",
    "    \"\"\"\n",
    "    Search for the specified value in the given column of the DataFrame.\n",
    "    \n",
    "    :param df: The DataFrame to search.\n",
    "    :param search_column: The column to search in.\n",
    "    :param search_value: The value to search for.\n",
    "    :return: Filtered DataFrame or None if no match is found.\n",
    "    \"\"\"\n",
    "    filtered_df = df[df[search_column] == search_value]\n",
    "    \n",
    "    if not filtered_df.empty:\n",
    "        return filtered_df.to_json(orient='records')\n",
    "    else:\n",
    "        print(f\"No match found for {search_value} in column {search_column}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "# Step 1: Parse the Excel file and save the DataFrame\n",
    "excel_file_path = 'files/Mediclaim FS E.1 v6.0.xlsx'\n",
    "sheet_name = 'MediClaim Business Rules' \n",
    "save_file_path = 'db/MediclaimFS.pkl'\n",
    "parse_and_save_excel(excel_file_path, sheet_name, save_file_path)\n",
    "\n",
    "# Step 2: Load the saved DataFrame\n",
    "loaded_df = load_df(save_file_path)\n",
    "\n",
    "# Step 3: Perform a search on the loaded DataFrame\n",
    "search_col = 'Error Code'\n",
    "search_val = 'VCA441'\n",
    "result_df = search_df(loaded_df, search_col, search_val)\n",
    "\n",
    "if result_df is not None:\n",
    "    print(result_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine document relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "#LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature = 0)\n",
    "\n",
    "prompt= PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are a grader assessing relevance of a retrieved document to a user question. \n",
    "    If the document contains keywords related to the user question, grade it as relevant.\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate wherther the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the retrieved document: \\n\\n {document}\\n\\n\n",
    "    Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\",\"document\"],\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST for retrieval grader\n",
    "\n",
    "question = \"what the reason behind error code VCA373 and write a playbook to investigate an error\"\n",
    "error_code = \"VCA373\"\n",
    "docs = retriever.invoke(question) + retriever.invoke(error_code)\n",
    "\n",
    "# Debug print statements\n",
    "print(\"Retrieved documents:\")\n",
    "print(docs)\n",
    "\n",
    "for ele in docs:\n",
    "    print(ele)\n",
    "\n",
    "# Ensure we have the correct index and document content\n",
    "if len(docs) > 1:\n",
    "    doc_text = docs[1]\n",
    "    print(\"Document text:\")\n",
    "    print(doc_text)\n",
    "\n",
    "    # Invoke the grader\n",
    "    result = retrieval_grader.invoke({\"question\": question, \"document\": doc_text})\n",
    "    print(\"Grader result:\")\n",
    "    print(result)\n",
    "else:\n",
    "    print(\"Insufficient documents retrieved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "#LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature = 0)\n",
    "\n",
    "prompt= PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are an indentifier.\n",
    "    Identify the error codes from the given question / sentence. An example could be VCA373 . Error codes usually start with a few capital alphabets followed by a few numbers\\n\n",
    "    It does not need to be a stringent test. The goal is to identify only the error codes and return that. \\n\n",
    "    It MUST have both the alpahbets and numbers together.\\n\n",
    "    Provide the error codes as a JSON with a single key 'error_code' and no preamble or explanation.\\n\n",
    "    Return the error codes if they are present, else return an the JSON with the value as an empty listt for example 'error_code' : []. \\n\n",
    "    Put the error codes in a list. For example 'error_code' : ['VCA373'] .\\n\n",
    "    If there are multiple error codes then for example 'error_code' : ['VCA373', 'CHC016'].\\n\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the sentence: \\n\\n {question}\\n\\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "error_identifier = prompt | llm | JsonOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_error_code(question):\n",
    "    error_code_JSON = error_identifier.invoke({\"question\": question})\n",
    "    print(\"error_code_JSON: \", error_code_JSON)\n",
    "    return error_code_JSON[\"error_code\"]\n",
    "\n",
    "### Document Retrieval and Combination\n",
    "def retrieve_and_combine_documents(question):\n",
    "    error_code_list = extract_error_code(question)\n",
    "    combined_docs = []\n",
    "\n",
    "    if error_code_list != []:\n",
    "        print(f\"Extracted Error Code: {error_code_list}\")\n",
    "\n",
    "        with open('db/healthcare.txt', 'r') as file1:\n",
    "            lines = file1.readlines()\n",
    "\n",
    "        with open('db/UCFMSG_validation_codes.txt', 'r') as file2:\n",
    "            lines += file2.readlines()\n",
    "\n",
    "        for error_index in range(0, len(error_code_list)):\n",
    "\n",
    "            matched_snippets = []\n",
    "            skip_lines = 0\n",
    "            skip = False\n",
    "            doc_count = 0\n",
    "\n",
    "            for i in range(0, len(lines)):\n",
    "\n",
    "                skip_lines -= 1\n",
    "\n",
    "                if error_code_list[error_index] in lines[i] and not skip:\n",
    "                    doc_count += 1\n",
    "                    start = max(0, i - 15)\n",
    "                    end = min(len(lines), i + 7)\n",
    "                    snippet = ''.join(lines[start:end])\n",
    "\n",
    "                    header = f\"Document {doc_count} for error {error_code_list[error_index]}:\\n\"\n",
    "            \n",
    "                    # Prepend the header to the snippet\n",
    "                    snippet_with_header = header + snippet\n",
    "                    \n",
    "                    # Append the modified snippet to the matched snippets list\n",
    "                    matched_snippets.append(snippet_with_header)\n",
    "\n",
    "                    skip_lines = 10\n",
    "                    skip = True\n",
    "\n",
    "                    if len(matched_snippets) >= 4:\n",
    "                        break\n",
    "\n",
    "                if skip_lines <= 0:\n",
    "                    skip = False\n",
    "                    skip = 0\n",
    "\n",
    "\n",
    "            result = matched_snippets[:4]\n",
    "            combined_docs.append(result)\n",
    "            print(result)\n",
    "\n",
    "            if result != []:\n",
    "                print(\"Exact match found in healthcare.txt for error\", error_code_list[error_index])\n",
    "                print(\"-------------------------------\")\n",
    "            else:\n",
    "                 print(f\"No Error Code {error_code_list[error_index]} found in the documents.\")\n",
    "\n",
    "    else:\n",
    "        print(\"No Error Code found in the question.\")\n",
    "\n",
    "    return combined_docs, error_code_list\n",
    "\n",
    "# # Usage Example\n",
    "# question = \"What is the reason behind error code CHC016?\"\n",
    "# combined_docs = retrieve_and_combine_documents(question, retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST for error identifier\n",
    "\n",
    "# Usage Example\n",
    "question = \"What is the reason behind error code CHC016 and VCA441?\"\n",
    "combined_docs, error_code_list = retrieve_and_combine_documents(question)\n",
    "\n",
    "# Debug print statements\n",
    "print(\"Retrieved documents:\")\n",
    "for i, doc in enumerate(combined_docs):\n",
    "    print(f\"combined doc {i}: {doc}\")\n",
    "\n",
    "# print(question)\n",
    "# print(combined_docs)\n",
    "# generation = rag_chain.invoke({\"context\": combined_docs, \"question\": question})\n",
    "# print(\"Generated Response:\")\n",
    "# print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST for main generation response\n",
    "question = \"What is the reason behind error CHC016\"\n",
    "combined_docs = retrieve_and_combine_documents(question, retriever)\n",
    "# Extract text from combined documents\n",
    "# context = \"\\n\\n\".join([doc for doc in combined_docs])\n",
    "\n",
    "generation = rag_chain.invoke({\"context\": combined_docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for answer for hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt= PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are a grader assessing whether an answer is grounded in / supported by a set of facts.\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the answer is grounded in / supported by a set of facts.\n",
    "    Provide the binary score as a JSON with a single ey 'score' and no preamble or explanation.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here are the facts: \n",
    "    \\n-------\\n \n",
    "    {documents}\n",
    "    \\n-------\\n\n",
    "    Here is the answer: {generation} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"generation\",\"document\"],\n",
    ")\n",
    "\n",
    "hallucination_grader = prompt | llm | JsonOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST for hallucination grader\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt= PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are a grader assessing whether an answer is addressing the question properly.\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the answer addresses the question well.\n",
    "    Provide the binary score as a JSON with a single ey 'score' and no preamble or explanation.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question}\n",
    "    Here is the answer: {generation} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"generation\",\"question\"],\n",
    ")\n",
    "\n",
    "answer_grader = prompt | llm | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST for answer grader\n",
    "answer_grader.invoke({\"question\": question, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from theme import JS_LIGHT_THEME, CSS\n",
    "from logger import Logger\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "logger_instance = Logger(\"logfile.txt\")\n",
    "\n",
    "\n",
    "prompt_template= PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are an assistant for question-answering tasks. Often, the context might be in code, descriptions, tables and JSON, do your best to analyse them.\n",
    "    You can try to identify the specific things mentioned in the question and work from there.\n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    If you dont know, also mention what you could discern specifically from the context as well as what you think you might need to answer the given question.\n",
    "    \n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question}\n",
    "    Context: {context}\n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\",\"context\"],\n",
    ")\n",
    "\n",
    "prompt_template_history= PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are an assistant for question-answering tasks. Often, the context might be in code, descriptions, tables and JSON, do your best to analyse them.\n",
    "    You can try to identify the specific things mentioned in the question and work from there.\n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    If you dont know, also mention what you could discern specifically from the context as well as what you think you might need to answer the given question.\n",
    "    If your Chat history is provided to you, please take into account the history of the chat and answer with it in mind.\n",
    "    The format of the chat history is a list of lists. An example is [[\"my question\", \"your response\"],[\"my question\", None]].\n",
    "    \n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question}\n",
    "    Context: {context}\n",
    "    Chat History: {history}\n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\",\"context\", \"history\"],\n",
    ")\n",
    "\n",
    "# After formatting the variable \"context\" given to you nicely into a human readable format, return it at the end in the format \"Context: (formatted context)\". \n",
    "#     If no \"context\" variable is given, dont need to add anything.\n",
    "\n",
    "# Prompt template for formatting context\n",
    "context_formatting_template = PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are an assistant for formatting tasks. Do not alter the content or add in any new content.\n",
    "    You are only in charge of reformatting the context given to you. \n",
    "    The context is parsed from a pdf and might be part of code, descriptions or tables. Do your best to format it nicely and do not format everything into code by adding \"```\" at the start. Only format into code when required such as for \"definition\".\n",
    "    At the end of the context, there should be a JSON string, format it into a table if suitable and ensure it is not in a '<code>' block.\n",
    "    Format the given \"context\" variable and return it in the format \"Context: (formatted context)\". \n",
    "    If no \"context\" variable is given, just return an empty string.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Context: {context}\n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"context\"],\n",
    ")\n",
    "\n",
    "# Initialize the LLM\n",
    "local_llm = \"llama3\"\n",
    "llm_main = ChatOllama(model=local_llm, temperature=0, callbacks=callback_manager)\n",
    "llm_formatter = ChatOllama(model=local_llm, temperature=0, callbacks=callback_manager)\n",
    "\n",
    "def format_history(msg, history):\n",
    "    chat_history = [{\"role\": \"system\", \"content\": \"hi\"}]\n",
    "    for query, response in history:\n",
    "        chat_history.append({\"role\": \"user\", \"content\": query})\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "    chat_history.append({\"role\": \"user\", \"content\": msg})\n",
    "    return chat_history\n",
    "\n",
    "def generate_response(message, history, top_k, top_p, temperature, chat_history=False, validate=False, check_hallucination=False, check_context=False):\n",
    "    if history is None:\n",
    "        history = []\n",
    "\n",
    "    print(\"history\", history)\n",
    "    validation_agent = retrieval_grader\n",
    "    hallucination_agent = hallucination_grader\n",
    "\n",
    "    # # Retrieve and combine documents\n",
    "    combined_docs, error_code_list = retrieve_and_combine_documents(message)\n",
    "\n",
    "    excel_result = \"\"\n",
    "    if error_code_list != []:\n",
    "        for error_index in range(0, len(error_code_list)):\n",
    "            excel_result += search_df(loaded_df, search_col , error_code_list[error_index])\n",
    "            excel_result += \"\\n\\n\"\n",
    "\n",
    "    combined_docs_string = \"\\n\\n\".join([str(doc) for doc in combined_docs]) + \"\\n\\n\" + excel_result\n",
    "\n",
    "    print(combined_docs_string)\n",
    "\n",
    "    # Generate the main response\n",
    "    if chat_history:\n",
    "        prompt = prompt_template_history.format(question=message, context=combined_docs_string, history=history)\n",
    "        print(\"prompting with history: \", history)\n",
    "    else:\n",
    "        prompt = prompt_template.format(question=message, context=combined_docs_string) \n",
    "\n",
    "    response = llm_main.stream(prompt, top_k = int(top_k), top_p = float(top_p), temperature = float(temperature))\n",
    "    result = \"\"\n",
    "    for partial_answer in response:\n",
    "        result += partial_answer.content\n",
    "        history[-1][1] = result\n",
    "        yield history, gr.update()  # Yield the main response first\n",
    "\n",
    "    # Append the context check result to the main response if requested\n",
    "    if check_context:\n",
    "        context_prompt = context_formatting_template.format(context=combined_docs_string)\n",
    "        context_response = llm_formatter.stream(context_prompt)\n",
    "        result += \"\\n\\nFormatted Context:\\n\"\n",
    "        for partial_context in context_response:\n",
    "            result += partial_context.content\n",
    "            history[-1][1] = result\n",
    "            yield history, gr.update()\n",
    "\n",
    "    # Perform validation check if requested\n",
    "    if validate:\n",
    "        documents = validation_agent.invoke({\"question\": message, \"document\": doc_text})\n",
    "        validation_result = \"Validation Check: Documents validated successfully.\"\n",
    "        result += \"\\n\\n\" + validation_result\n",
    "        history[-1][1] = result\n",
    "        yield history, gr.update()\n",
    "\n",
    "    # Perform hallucination check if requested\n",
    "    if check_hallucination:\n",
    "        hallucination_result = hallucination_agent.invoke({\"documents\": combined_docs, \"generation\": result})\n",
    "        if hallucination_result:\n",
    "            hallucination_result_text = \"\\n\\nHallucination Check:\\n\" + hallucination_result\n",
    "            result += hallucination_result_text\n",
    "            history[-1][1] = result\n",
    "            yield history, gr.update()\n",
    "\n",
    "    history[-1][1] = result\n",
    "    yield history, gr.update()\n",
    "\n",
    "def add_message(history, message):\n",
    "    if message is not None:\n",
    "        history.append([message, None])\n",
    "    return history, gr.update(value=\"\")\n",
    "\n",
    "with gr.Blocks(\n",
    "    theme=gr.themes.Soft(primary_hue=\"slate\"),\n",
    "    js=JS_LIGHT_THEME,\n",
    "    css=CSS,\n",
    ") as demo:\n",
    "    gr.Markdown(\"## Day2Ops Chatbot ðŸ¤–\")\n",
    "\n",
    "    with gr.Tab(\"Interface\"):\n",
    "        sidebar_state = gr.State(True)\n",
    "        with gr.Row():\n",
    "            with gr.Column(\n",
    "                variant=\"panel\", scale=10, visible=sidebar_state.value\n",
    "            ) as setting:\n",
    "                with gr.Column():\n",
    "                    status = gr.Textbox(\n",
    "                        label=\"Status\", value=\"Ready!\", interactive=False\n",
    "                    )\n",
    "                    language = gr.Radio(\n",
    "                        label=\"Language\",\n",
    "                        choices=[\"eng\"],\n",
    "                        value=\"eng\",\n",
    "                        interactive=True,\n",
    "                    )\n",
    "                    model = gr.Dropdown(\n",
    "                        label=\"Choose Model:\",\n",
    "                        choices=[\n",
    "                            \"llama3:latest\",\n",
    "                        ],\n",
    "                        value=\"llama3:latest\",\n",
    "                        interactive=True,\n",
    "                        allow_custom_value=True,\n",
    "                    )\n",
    "                    chat_history = gr.Checkbox(label=\"Enable message memory (takes longer the more you query)\")\n",
    "                    context_checkbox = gr.Checkbox(label=\"Show Context (takes longer)\")\n",
    "                    validate_checkbox = gr.Checkbox(label=\"Validate Documents if relevant (takes longer)\")\n",
    "                    hallucination_checkbox = gr.Checkbox(label=\"Check if there is hallucination of answer (takes longer)\")\n",
    "                    check_answer_checkbox = gr.Checkbox(label=\"Check answer if relevant to question (takes longer)\")\n",
    "\n",
    "            with gr.Column(scale=30, variant=\"panel\"):\n",
    "                chatbot = gr.Chatbot(\n",
    "                    elem_id=\"chatbot\",\n",
    "                    layout=\"bubble\",\n",
    "                    value=[],\n",
    "                    height=550,\n",
    "                    scale=2,\n",
    "                    show_copy_button=True,\n",
    "                    bubble_full_width=False,\n",
    "                    avatar_images=(\"../assets/user.png\",\"../assets/bot.png\")\n",
    "                )\n",
    "                with gr.Row():\n",
    "                    message_input = gr.Textbox(\n",
    "                        placeholder=\"Enter message...\", \n",
    "                        show_label=False, \n",
    "                        scale=3, \n",
    "                        lines=1\n",
    "                    )\n",
    "\n",
    "                    submit_btn = gr.Button(\"Submit\",scale=1)\n",
    "                \n",
    "\n",
    "                with gr.Row(variant=\"panel\"):\n",
    "                    undo_btn = gr.Button(value=\"Undo\", min_width=20)\n",
    "                    clear_btn = gr.Button(value=\"Clear\", min_width=20)\n",
    "                    reset_btn = gr.Button(value=\"Reset\", min_width=20)\n",
    "\n",
    "\n",
    "    with gr.Tab(\"Settings\"):\n",
    "        with gr.Row():\n",
    "            top_k = gr.Slider(0.0,100.0, label=\"top_k\", value=40, info=\"Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)\")\n",
    "            top_p = gr.Slider(0.0,1.0, label=\"top_p\", value=0.9, info=\" Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)\")\n",
    "            temp = gr.Slider(0.0,2.0, label=\"temperature\", value=0.8, info=\"The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8)\")\n",
    "\n",
    "\n",
    "    with gr.Tab(\"Output\"):\n",
    "        with gr.Row(variant=\"panel\"):\n",
    "            log = gr.Code(\n",
    "                label=\"\", language=\"markdown\", interactive=False, lines=30\n",
    "            )\n",
    "            demo.load(\n",
    "                logger_instance.read_logs,\n",
    "                outputs=[log],\n",
    "                every=1,\n",
    "                show_progress=\"hidden\",\n",
    "                scroll_to_output=True,\n",
    "            )\n",
    "                \n",
    "    state = gr.State([])  # Initialize state as an empty list to hold the chat history\n",
    "\n",
    "    def gradio_chat_ollama(history, top_k, top_p, temperature, chat_history, validate, check_hallucination, check_context):\n",
    "        message = history[-1][0] if history else \"\"\n",
    "        generator = generate_response(message, history, top_k, top_p, temperature, chat_history, validate, check_hallucination, check_context)\n",
    "        for response in generator:\n",
    "            yield response\n",
    "\n",
    "    chat_msg = message_input.submit(add_message, [state, message_input], [state, chatbot]\n",
    "    ).then(\n",
    "        lambda: \"Processing...\", None, status\n",
    "    ).then(\n",
    "        gradio_chat_ollama, [state, top_k, top_p, temp, chat_history, validate_checkbox, hallucination_checkbox, context_checkbox], [chatbot, state]\n",
    "    ).then(\n",
    "        lambda: \"\", None, message_input  # Clear the textbox\n",
    "    ).then(\n",
    "        lambda: \"Completed!\", None, status\n",
    "    )\n",
    "\n",
    "    submit_btn.click(add_message, [state, message_input], [state, chatbot]\n",
    "    ).then(\n",
    "        lambda: \"Processing...\", None, status\n",
    "    ).then(\n",
    "        gradio_chat_ollama, [state, top_k, top_p, temp, chat_history, validate_checkbox, hallucination_checkbox, context_checkbox], [chatbot, state]\n",
    "    ).then(\n",
    "        lambda: \"\", None, message_input  # Clear the textbox\n",
    "    ).then(\n",
    "        lambda: \"Completed!\", None, status\n",
    "    )\n",
    "    \n",
    "    clear_btn.click(lambda: [], None, chatbot, queue=False)  # Clear the chat\n",
    "\n",
    "    \n",
    "\n",
    "    demo.queue()\n",
    "    demo.launch(show_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "role, task ,format?\n",
    "\n",
    "any fields to for MI to check first"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
