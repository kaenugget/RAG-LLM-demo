{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm=\"llama3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01758289337158203,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Fetching 5 files",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "873fb45a07e54ca4a001dd0564484dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever created successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "\n",
    "def initialize_vector_store():\n",
    "    # Step 1: Read the contents of documents.txt\n",
    "    with open('db/documents.txt', 'r') as file:\n",
    "        documents = file.readlines()\n",
    "\n",
    "    # Ensure the documents are in the right format (list of strings)\n",
    "    documents = [doc.strip() for doc in documents]\n",
    "\n",
    "    # Step 2: Initialize the embedding model\n",
    "    embedding = FastEmbedEmbeddings()\n",
    "\n",
    "    # Step 3: Initialize the Chroma vector store with the embedding function\n",
    "    persist_directory = \"db\"\n",
    "    if not os.path.exists(persist_directory):\n",
    "        os.makedirs(persist_directory)\n",
    "\n",
    "    vector_store = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "\n",
    "    # Step 4: Add documents to the vector store\n",
    "    for document in documents:\n",
    "        vector_store.add_texts([document])\n",
    "\n",
    "    return vector_store\n",
    "\n",
    "def get_retriever():\n",
    "    persist_directory = \"db\"\n",
    "    \n",
    "    # Check if vector store already exists\n",
    "    if not os.path.exists(persist_directory):\n",
    "        vector_store = initialize_vector_store()\n",
    "    else:\n",
    "        # Initialize Chroma vector store without re-adding texts\n",
    "        embedding = FastEmbedEmbeddings()\n",
    "        vector_store = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "\n",
    "    # Step 5: Create the retriever\n",
    "    retriever = vector_store.as_retriever()\n",
    "    return retriever\n",
    "\n",
    "# Example usage\n",
    "retriever = get_retriever()\n",
    "print(\"Retriever created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'VectorStoreRetriever' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage of the retriever\u001b[39;00m\n\u001b[1;32m      2\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour query here\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mretriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'VectorStoreRetriever' object is not callable"
     ]
    }
   ],
   "source": [
    "# Example usage of the retriever\n",
    "query = \"Your query here\"\n",
    "results = retriever(query)\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine document relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "#LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature = 0)\n",
    "\n",
    "prompt= PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are a grader assessing relevance of a retrieved document to a user question. \n",
    "    If the document contains keywords related to the user question, grade it as relevant.\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate wherther the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the retrieved document: \\n\\n {document}\\n\\n\n",
    "    Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\",\"document\"],\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST for retrieval grader\n",
    "\n",
    "question = \"what the reason behind error code VCA373 and write a playbook to investigate an error\"\n",
    "error_code = \"VCA373\"\n",
    "docs = retriever.invoke(question) + retriever.invoke(error_code)\n",
    "\n",
    "# Debug print statements\n",
    "print(\"Retrieved documents:\")\n",
    "print(docs)\n",
    "\n",
    "for ele in docs:\n",
    "    print(ele)\n",
    "\n",
    "# Ensure we have the correct index and document content\n",
    "if len(docs) > 1:\n",
    "    doc_text = docs[1]\n",
    "    print(\"Document text:\")\n",
    "    print(doc_text)\n",
    "\n",
    "    # Invoke the grader\n",
    "    result = retrieval_grader.invoke({\"question\": question, \"document\": doc_text})\n",
    "    print(\"Grader result:\")\n",
    "    print(result)\n",
    "else:\n",
    "    print(\"Insufficient documents retrieved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "#LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature = 0)\n",
    "\n",
    "prompt= PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are an indentifier.\n",
    "    Identify the error code from the given question / sentence. An example could be VCA373 . Error codes usually start with a few capital alphabets followed by a few numbers\n",
    "    It does not need to be a stringent test. The goal is to identify only the error code and return that. \\n\n",
    "    Return the error code if it is present, else return an empty string ''. \\n\n",
    "    Provide the error code as a JSON with a single key 'error_code' and no preamble or explanation.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the sentence: \\n\\n {question}\\n\\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "error_identifier = prompt | llm | JsonOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_error_code(question):\n",
    "    error_code_JSON = error_identifier.invoke({\"question\": question})\n",
    "    return error_code_JSON[\"error_code\"]\n",
    "\n",
    "### Document Retrieval and Combination\n",
    "def retrieve_and_combine_documents(question, retriever):\n",
    "    error_code = extract_error_code(question)\n",
    "    combined_docs = []\n",
    "\n",
    "    if error_code:\n",
    "        print(f\"Extracted Error Code: {error_code}\")\n",
    "\n",
    "        with open('db/documents.txt', 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        matched_snippets = set()\n",
    "        skip_lines = 0\n",
    "        skip = False\n",
    "        for i, line in enumerate(lines):\n",
    "\n",
    "            skip_lines -= 1\n",
    "\n",
    "            if error_code in line and not skip:\n",
    "                start = max(0, i - 10)\n",
    "                end = min(len(lines), i + 11)\n",
    "                snippet = ''.join(lines[start:end])\n",
    "                matched_snippets.add(snippet)\n",
    "\n",
    "                skip_lines = 10\n",
    "                skip = True\n",
    "\n",
    "                if len(matched_snippets) >= 5:\n",
    "                    break\n",
    "\n",
    "            if skip_lines <= 0:\n",
    "                skip = False\n",
    "                skip = 0\n",
    "\n",
    "\n",
    "\n",
    "        combined_docs = list(matched_snippets)[:5]\n",
    "        print(\"Exact match found in documents.txt\")\n",
    "        print(\"-------------------------------\")\n",
    "        for idx, doc in enumerate(combined_docs):\n",
    "            print(f\"combined_doc {idx}: {doc}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No Error Code found in the question.\")\n",
    "        # Retrieve documents only for the general question if no error code is found\n",
    "        combined_docs = retriever(question)\n",
    "\n",
    "    return combined_docs\n",
    "\n",
    "# # Usage Example\n",
    "# question = \"What is the reason behind error code CHC016?\"\n",
    "# combined_docs = retrieve_and_combine_documents(question, retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST for error identifier\n",
    "\n",
    "# Usage Example\n",
    "question = \"What is the reason behind error code CHC016?\"\n",
    "combined_docs = retrieve_and_combine_documents(question, retriever)\n",
    "\n",
    "# Debug print statements\n",
    "print(\"Retrieved documents:\")\n",
    "for i, doc in enumerate(combined_docs):\n",
    "    print(f\"combined doc {i}: {doc.page_content}\")\n",
    "\n",
    "# print(question)\n",
    "# print(combined_docs)\n",
    "# generation = rag_chain.invoke({\"context\": combined_docs, \"question\": question})\n",
    "# print(\"Generated Response:\")\n",
    "# print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "prompt= PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are an assistant for question-answering tasks. Often, the context might be in code or descriptions, do your best to format it nicely and then analyse them.\n",
    "    You can try to identify the specific things mentioned in the question and work from there.\n",
    "    After formatting it nicely into a human readable format, return it at the end in the format \"Context: (your formatted context)\"\n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    If you dont know, also mention what you could discern specifically from the context as well as what you think you might need to answer the given question.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question}\n",
    "    Context: {context}\n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\",\"context\"],\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=local_llm, temperature=0, callbacks=callback_manager)\n",
    "\n",
    "#chain\n",
    "\n",
    "rag_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST for main generation response\n",
    "question = \"What is the reason behind error CHC016\"\n",
    "combined_docs = retrieve_and_combine_documents(question, retriever)\n",
    "# Extract text from combined documents\n",
    "# context = \"\\n\\n\".join([doc for doc in combined_docs])\n",
    "\n",
    "generation = rag_chain.invoke({\"context\": combined_docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for answer for hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt= PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are a grader assessing whether an answer is grounded in / supported by a set of facts.\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the answer is grounded in / supported by a set of facts.\n",
    "    Provide the binary score as a JSON with a single ey 'score' and no preamble or explanation.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here are the facts: \n",
    "    \\n-------\\n \n",
    "    {documents}\n",
    "    \\n-------\\n\n",
    "    Here is the answer: {generation} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"generation\",\"document\"],\n",
    ")\n",
    "\n",
    "hallucination_grader = prompt | llm | JsonOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST for hallucination grader\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt= PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are a grader assessing whether an answer is addressing the question properly.\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the answer addresses the question well.\n",
    "    Provide the binary score as a JSON with a single ey 'score' and no preamble or explanation.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question}\n",
    "    Here is the answer: {generation} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"generation\",\"question\"],\n",
    ")\n",
    "\n",
    "answer_grader = prompt | llm | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST for answer grader\n",
    "answer_grader.invoke({\"question\": question, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context generator\n",
    "\n",
    "# Prompt template for formatting context\n",
    "context_formatting_template = PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are an assistant for formatting tasks. Do not alter the content or add in any new content.\n",
    "    You are only in charge of reformatting the context given to you. \n",
    "    The context is parsed from a pdf and might be part of code, descriptions or tables. Do your best to format it nicely into markdown.\n",
    "    Format the given \"context\" variable and return it in the format \"Context: (formatted context)\". \n",
    "    If no \"context\" variable is given, just return an empty string.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Context: {context}\n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"context\"],\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=local_llm, temperature=0, callbacks=callback_manager)\n",
    "context_formatter = prompt | llm | StrOutputParser()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "\n",
    "prompt_template= PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are an assistant for question-answering tasks. Often, the context might be in code, descriptions or tables, do your best to format it nicely and then analyse them.\n",
    "    You can try to identify the specific things mentioned in the question and work from there.\n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    If you dont know, also mention what you could discern specifically from the context as well as what you think you might need to answer the given question.\n",
    "    \n",
    "    \n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question}\n",
    "    Context: {context}\n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\",\"context\"],\n",
    ")\n",
    "\n",
    "# After formatting the variable \"context\" given to you nicely into a human readable format, return it at the end in the format \"Context: (formatted context)\". \n",
    "#     If no \"context\" variable is given, dont need to add anything.\n",
    "\n",
    "# Prompt template for formatting context\n",
    "context_formatting_template = PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are an assistant for formatting tasks. Do not alter the content or add in any new content.\n",
    "    You are only in charge of reformatting the context given to you. \n",
    "    The context is parsed from a pdf and might be part of code, descriptions or tables. Do your best to format it nicely into markdown.\n",
    "    Format the given \"context\" variable and return it in the format \"Context: (formatted context)\". \n",
    "    If no \"context\" variable is given, just return an empty string.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Context: {context}\n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"context\"],\n",
    ")\n",
    "\n",
    "# Initialize the LLM\n",
    "local_llm = \"llama3\"\n",
    "llm = ChatOllama(model=local_llm, temperature=0, callbacks=callback_manager)\n",
    "llm_formatter = ChatOllama(model=local_llm, temperature=0, callbacks=callback_manager)\n",
    "\n",
    "def format_history(msg, history):\n",
    "    chat_history = [{\"role\": \"system\", \"content\": \"hi\"}]\n",
    "    for query, response in history:\n",
    "        chat_history.append({\"role\": \"user\", \"content\": query})\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "    chat_history.append({\"role\": \"user\", \"content\": msg})\n",
    "    return chat_history\n",
    "\n",
    "def generate_response(message, history, validate=False, check_hallucination=False, check_context=False):\n",
    "    if history is None:\n",
    "        history = []\n",
    "\n",
    "    validation_agent = retrieval_grader\n",
    "    hallucination_agent = hallucination_grader\n",
    "\n",
    "    # Retrieve and combine documents\n",
    "    combined_docs = retrieve_and_combine_documents(message, retriever)\n",
    "    combined_docs_string = \"\\n\\n\".join([str(doc) for doc in combined_docs])\n",
    "\n",
    "    # Generate the main response\n",
    "    prompt = prompt_template.format(question=message, context=combined_docs_string)\n",
    "    response = llm.stream(prompt)\n",
    "    result = \"\"\n",
    "    for partial_answer in response:\n",
    "        result += partial_answer.content\n",
    "        history[-1][1] = result\n",
    "        yield history, gr.update()  # Yield the main response first\n",
    "\n",
    "    # Append the context check result to the main response if requested\n",
    "    if check_context:\n",
    "        context_prompt = context_formatting_template.format(context=combined_docs_string)\n",
    "        context_response = llm_formatter.stream(context_prompt)\n",
    "        result += \"\\n\\nFormatted Context:\\n\"\n",
    "        for partial_context in context_response:\n",
    "            result += partial_context.content\n",
    "            history[-1][1] = result\n",
    "            yield history, gr.update()\n",
    "\n",
    "    # Perform validation check if requested\n",
    "    if validate:\n",
    "        documents = validation_agent.invoke({\"question\": message, \"document\": doc_text})\n",
    "        validation_result = \"Validation Check: Documents validated successfully.\"\n",
    "        result += \"\\n\\n\" + validation_result\n",
    "        history[-1][1] = result\n",
    "        yield history, gr.update()\n",
    "\n",
    "    # Perform hallucination check if requested\n",
    "    if check_hallucination:\n",
    "        hallucination_result = hallucination_agent.invoke({\"documents\": combined_docs, \"generation\": result})\n",
    "        if hallucination_result:\n",
    "            hallucination_result_text = \"\\n\\nHallucination Check:\\n\" + hallucination_result\n",
    "            result += hallucination_result_text\n",
    "            history[-1][1] = result\n",
    "            yield history, gr.update()\n",
    "\n",
    "    history[-1][1] = result\n",
    "    yield history, gr.update()\n",
    "\n",
    "def add_message(history, message):\n",
    "    if message is not None:\n",
    "        history.append([message, None])\n",
    "    return history, gr.update(value=\"\")\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot([], elem_id=\"chatbot\", bubble_full_width=False)\n",
    "    message_input = gr.Textbox(placeholder=\"Enter message...\", show_label=False)\n",
    "    context_checkbox = gr.Checkbox(label=\"Show Context\")\n",
    "    validate_checkbox = gr.Checkbox(label=\"Validate Documents\")\n",
    "    hallucination_checkbox = gr.Checkbox(label=\"Check Hallucination\")\n",
    "    clear_btn = gr.Button(\"Clear\")\n",
    "    state = gr.State([])  # Initialize state as an empty list to hold the chat history\n",
    "\n",
    "    def gradio_chat_ollama(history, validate, check_hallucination, check_context):\n",
    "        message = history[-1][0] if history else \"\"\n",
    "        generator = generate_response(message, history, validate, check_hallucination, check_context)\n",
    "        for response in generator:\n",
    "            yield response\n",
    "\n",
    "    chat_msg = message_input.submit(add_message, [state, message_input], [state, chatbot]).then(\n",
    "        gradio_chat_ollama, [state, validate_checkbox, hallucination_checkbox, context_checkbox], [chatbot, state]\n",
    "    )\n",
    "    clear_btn.click(lambda: [], None, chatbot, queue=False)  # Clear the chat\n",
    "\n",
    "    with gr.Row():\n",
    "        top_k = gr.Slider(0.0,100.0, label=\"top_k\", value=40, info=\"Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)\")\n",
    "        top_p = gr.Slider(0.0,1.0, label=\"top_p\", value=0.9, info=\" Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)\")\n",
    "        temp = gr.Slider(0.0,2.0, label=\"temperature\", value=0.8, info=\"The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8)\")\n",
    "\n",
    "\n",
    "    demo.queue()\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
