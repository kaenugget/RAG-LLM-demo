{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm=\"llama3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01846003532409668,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Fetching 5 files",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c3ce74ee8e4d1eb731be8b46bcb671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever created successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "\n",
    "def initialize_vector_store():\n",
    "    # Step 1: Read the contents of documents.txt\n",
    "    with open('db/documents.txt', 'r') as file:\n",
    "        documents = file.readlines()\n",
    "\n",
    "    # Ensure the documents are in the right format (list of strings)\n",
    "    documents = [doc.strip() for doc in documents]\n",
    "\n",
    "    # Step 2: Initialize the embedding model\n",
    "    embedding = FastEmbedEmbeddings()\n",
    "\n",
    "    # Step 3: Initialize the Chroma vector store with the embedding function\n",
    "    persist_directory = \"db\"\n",
    "    if not os.path.exists(persist_directory):\n",
    "        os.makedirs(persist_directory)\n",
    "\n",
    "    vector_store = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "\n",
    "    # Step 4: Add documents to the vector store\n",
    "    for document in documents:\n",
    "        vector_store.add_texts([document])\n",
    "\n",
    "    return vector_store\n",
    "\n",
    "def get_retriever():\n",
    "    persist_directory = \"db\"\n",
    "    \n",
    "    # Check if vector store already exists\n",
    "    if not os.path.exists(persist_directory):\n",
    "        vector_store = initialize_vector_store()\n",
    "    else:\n",
    "        # Initialize Chroma vector store without re-adding texts\n",
    "        embedding = FastEmbedEmbeddings()\n",
    "        vector_store = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "\n",
    "    # Step 5: Create the retriever\n",
    "    retriever = vector_store.as_retriever()\n",
    "    return retriever\n",
    "\n",
    "# Example usage\n",
    "retriever = get_retriever()\n",
    "print(\"Retriever created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the retriever\n",
    "query = \"Your query here\"\n",
    "results = retriever(query)\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame from sheet 'MediClaim Business Rules' saved to db/MediclaimFS.pkl\n",
      "[{\"S\\/N\":416,\"Field Name\":\"Hospital Code\\nCharge Code\\nDate of Discharge\\nDate of Admission\",\"Business Rule\":\"If Hospital Code = (W1-W6, R00-RZZ) and charge code = RS0001 or RP0001, then Date of Discharge must be before 1 Jan 2023 and Bill Category must be DH.\\n\\nIf Hospital Code =  (W1-W6, R00-RZZ) and charge code = RS0002 - 4, then Date of Admission must be on or after 1 Jan 2023 and Bill Category must be OU.\",\"Error Code\":\"VCA441\",\"Error Description\":\"For day rehabilitation centres, RS0001\\/RP0001 should only be submitted with Bill Category DH and Date of Discharge before 1 Jan 2023. RS0002 to RS0004 should only be submitted with Bill Category OU and Date of Admission on or after 1 Jan 2023.\",\"Associated Policy\":\"SR-MediClaim-202207-0017(MediClaim - Changes to MSV DRC Rehab Limit)\",\"For Claim\\/ Non-Claim\":\"All\",\"Tag\":\"DRC\",\"Removed\":null}]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def parse_and_save_excel(file_path, sheet_name, save_path):\n",
    "    \"\"\"\n",
    "    Parse a specific sheet in the Excel file and save the DataFrame to a file.\n",
    "    \n",
    "    :param file_path: Path to the Excel file.\n",
    "    :param sheet_name: Name or index of the sheet to parse.\n",
    "    :param save_path: Path to save the DataFrame.\n",
    "    \"\"\"\n",
    "    # Load the specific sheet into a DataFrame\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "    \n",
    "    # Save the DataFrame to a file\n",
    "    with open(save_path, 'wb') as file:\n",
    "        pickle.dump(df, file)\n",
    "    print(f\"DataFrame from sheet '{sheet_name}' saved to {save_path}\")\n",
    "\n",
    "def load_df(save_path):\n",
    "    \"\"\"\n",
    "    Load the saved DataFrame from a file.\n",
    "    \n",
    "    :param save_path: Path to the saved DataFrame.\n",
    "    :return: Loaded DataFrame.\n",
    "    \"\"\"\n",
    "    with open(save_path, 'rb') as file:\n",
    "        df = pickle.load(file)\n",
    "    return df\n",
    "\n",
    "def search_df(df, search_column, search_value):\n",
    "    \"\"\"\n",
    "    Search for the specified value in the given column of the DataFrame.\n",
    "    \n",
    "    :param df: The DataFrame to search.\n",
    "    :param search_column: The column to search in.\n",
    "    :param search_value: The value to search for.\n",
    "    :return: Filtered DataFrame or None if no match is found.\n",
    "    \"\"\"\n",
    "    filtered_df = df[df[search_column] == search_value]\n",
    "    \n",
    "    if not filtered_df.empty:\n",
    "        return filtered_df.to_json(orient='records')\n",
    "    else:\n",
    "        print(f\"No match found for {search_value} in column {search_column}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "# Step 1: Parse the Excel file and save the DataFrame\n",
    "excel_file_path = 'files/Mediclaim FS E.1 v6.0.xlsx'\n",
    "sheet_name = 'MediClaim Business Rules' \n",
    "save_file_path = 'db/MediclaimFS.pkl'\n",
    "parse_and_save_excel(excel_file_path, sheet_name, save_file_path)\n",
    "\n",
    "# Step 2: Load the saved DataFrame\n",
    "loaded_df = load_df(save_file_path)\n",
    "\n",
    "# Step 3: Perform a search on the loaded DataFrame\n",
    "search_col = 'Error Code'\n",
    "search_val = 'VCA441'\n",
    "result_df = search_df(loaded_df, search_col, search_val)\n",
    "\n",
    "if result_df is not None:\n",
    "    print(result_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine document relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "#LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature = 0)\n",
    "\n",
    "prompt= PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are a grader assessing relevance of a retrieved document to a user question. \n",
    "    If the document contains keywords related to the user question, grade it as relevant.\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate wherther the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the retrieved document: \\n\\n {document}\\n\\n\n",
    "    Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\",\"document\"],\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST for retrieval grader\n",
    "\n",
    "question = \"what the reason behind error code VCA373 and write a playbook to investigate an error\"\n",
    "error_code = \"VCA373\"\n",
    "docs = retriever.invoke(question) + retriever.invoke(error_code)\n",
    "\n",
    "# Debug print statements\n",
    "print(\"Retrieved documents:\")\n",
    "print(docs)\n",
    "\n",
    "for ele in docs:\n",
    "    print(ele)\n",
    "\n",
    "# Ensure we have the correct index and document content\n",
    "if len(docs) > 1:\n",
    "    doc_text = docs[1]\n",
    "    print(\"Document text:\")\n",
    "    print(doc_text)\n",
    "\n",
    "    # Invoke the grader\n",
    "    result = retrieval_grader.invoke({\"question\": question, \"document\": doc_text})\n",
    "    print(\"Grader result:\")\n",
    "    print(result)\n",
    "else:\n",
    "    print(\"Insufficient documents retrieved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "#LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature = 0)\n",
    "\n",
    "prompt= PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are an indentifier.\n",
    "    Identify the error code from the given question / sentence. An example could be VCA373 . Error codes usually start with a few capital alphabets followed by a few numbers\n",
    "    It does not need to be a stringent test. The goal is to identify only the error code and return that. \\n\n",
    "    Return the error code if it is present, else return an empty string ''. \\n\n",
    "    Provide the error code as a JSON with a single key 'error_code' and no preamble or explanation.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the sentence: \\n\\n {question}\\n\\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "error_identifier = prompt | llm | JsonOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_error_code(question):\n",
    "    error_code_JSON = error_identifier.invoke({\"question\": question})\n",
    "    return error_code_JSON[\"error_code\"]\n",
    "\n",
    "### Document Retrieval and Combination\n",
    "def retrieve_and_combine_documents(question, retriever):\n",
    "    error_code = extract_error_code(question)\n",
    "    combined_docs = []\n",
    "\n",
    "    if error_code:\n",
    "        print(f\"Extracted Error Code: {error_code}\")\n",
    "\n",
    "        with open('db/documents.txt', 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        matched_snippets = set()\n",
    "        skip_lines = 0\n",
    "        skip = False\n",
    "        for i, line in enumerate(lines):\n",
    "\n",
    "            skip_lines -= 1\n",
    "\n",
    "            if error_code in line and not skip:\n",
    "                start = max(0, i - 10)\n",
    "                end = min(len(lines), i + 11)\n",
    "                snippet = ''.join(lines[start:end])\n",
    "                matched_snippets.add(snippet)\n",
    "\n",
    "                skip_lines = 10\n",
    "                skip = True\n",
    "\n",
    "                if len(matched_snippets) >= 5:\n",
    "                    break\n",
    "\n",
    "            if skip_lines <= 0:\n",
    "                skip = False\n",
    "                skip = 0\n",
    "\n",
    "\n",
    "\n",
    "        combined_docs = list(matched_snippets)[:5]\n",
    "        print(\"Exact match found in documents.txt\")\n",
    "        print(\"-------------------------------\")\n",
    "        for idx, doc in enumerate(combined_docs):\n",
    "            print(f\"combined_doc {idx}: {doc}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No Error Code found in the question.\")\n",
    "        # Retrieve documents only for the general question if no error code is found\n",
    "        combined_docs = retriever(question)\n",
    "\n",
    "    return combined_docs\n",
    "\n",
    "# # Usage Example\n",
    "# question = \"What is the reason behind error code CHC016?\"\n",
    "# combined_docs = retrieve_and_combine_documents(question, retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST for error identifier\n",
    "\n",
    "# Usage Example\n",
    "question = \"What is the reason behind error code CHC016?\"\n",
    "combined_docs = retrieve_and_combine_documents(question, retriever)\n",
    "\n",
    "# Debug print statements\n",
    "print(\"Retrieved documents:\")\n",
    "for i, doc in enumerate(combined_docs):\n",
    "    print(f\"combined doc {i}: {doc.page_content}\")\n",
    "\n",
    "# print(question)\n",
    "# print(combined_docs)\n",
    "# generation = rag_chain.invoke({\"context\": combined_docs, \"question\": question})\n",
    "# print(\"Generated Response:\")\n",
    "# print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "prompt= PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are an assistant for question-answering tasks. Often, the context might be in code or descriptions, do your best to format it nicely and then analyse them.\n",
    "    You can try to identify the specific things mentioned in the question and work from there.\n",
    "    After formatting it nicely into a human readable format, return it at the end in the format \"Context: (your formatted context)\"\n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    If you dont know, also mention what you could discern specifically from the context as well as what you think you might need to answer the given question.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question}\n",
    "    Context: {context}\n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\",\"context\"],\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=local_llm, temperature=0, callbacks=callback_manager)\n",
    "\n",
    "#chain\n",
    "\n",
    "rag_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST for main generation response\n",
    "question = \"What is the reason behind error CHC016\"\n",
    "combined_docs = retrieve_and_combine_documents(question, retriever)\n",
    "# Extract text from combined documents\n",
    "# context = \"\\n\\n\".join([doc for doc in combined_docs])\n",
    "\n",
    "generation = rag_chain.invoke({\"context\": combined_docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for answer for hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt= PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are a grader assessing whether an answer is grounded in / supported by a set of facts.\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the answer is grounded in / supported by a set of facts.\n",
    "    Provide the binary score as a JSON with a single ey 'score' and no preamble or explanation.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here are the facts: \n",
    "    \\n-------\\n \n",
    "    {documents}\n",
    "    \\n-------\\n\n",
    "    Here is the answer: {generation} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"generation\",\"document\"],\n",
    ")\n",
    "\n",
    "hallucination_grader = prompt | llm | JsonOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST for hallucination grader\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt= PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are a grader assessing whether an answer is addressing the question properly.\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the answer addresses the question well.\n",
    "    Provide the binary score as a JSON with a single ey 'score' and no preamble or explanation.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question}\n",
    "    Here is the answer: {generation} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"generation\",\"question\"],\n",
    ")\n",
    "\n",
    "answer_grader = prompt | llm | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST for answer grader\n",
    "answer_grader.invoke({\"question\": question, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context generator\n",
    "\n",
    "# Prompt template for formatting context\n",
    "context_formatting_template = PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are an assistant for formatting tasks. Do not alter the content or add in any new content.\n",
    "    You are only in charge of reformatting the context given to you. \n",
    "    The context is parsed from a pdf and might be part of code, descriptions or tables. Do your best to format it nicely into markdown.\n",
    "    Format the given \"context\" variable and return it in the format \"Context: (formatted context)\". \n",
    "    If no \"context\" variable is given, just return an empty string.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Context: {context}\n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"context\"],\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=local_llm, temperature=0, callbacks=callback_manager)\n",
    "context_formatter = prompt | llm | StrOutputParser()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7874\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7874/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"S\\/N\":416,\"Field Name\":\"Hospital Code\\nCharge Code\\nDate of Discharge\\nDate of Admission\",\"Business Rule\":\"If Hospital Code = (W1-W6, R00-RZZ) and charge code = RS0001 or RP0001, then Date of Discharge must be before 1 Jan 2023 and Bill Category must be DH.\\n\\nIf Hospital Code =  (W1-W6, R00-RZZ) and charge code = RS0002 - 4, then Date of Admission must be on or after 1 Jan 2023 and Bill Category must be OU.\",\"Error Code\":\"VCA441\",\"Error Description\":\"For day rehabilitation centres, RS0001\\/RP0001 should only be submitted with Bill Category DH and Date of Discharge before 1 Jan 2023. RS0002 to RS0004 should only be submitted with Bill Category OU and Date of Admission on or after 1 Jan 2023.\",\"Associated Policy\":\"SR-MediClaim-202207-0017(MediClaim - Changes to MSV DRC Rehab Limit)\",\"For Claim\\/ Non-Claim\":\"All\",\"Tag\":\"DRC\",\"Removed\":null}]\n",
      "VCA441 is an error code that appears in a specific context related to medical claims and billing. Here's a breakdown of what I can infer from the provided context:\n",
      "\n",
      "* The error code VCA441 is associated with a business rule that governs how day rehabilitation centers (DRC) submit their claims.\n",
      "* The rule states that when the hospital code falls within certain ranges (W1-W6, R00-RZZ) and the charge code is either RS0001 or RP0001, then the date of discharge must be before January 1, 2023, and the bill category must be \"DH\".\n",
      "* Conversely, if the charge code is between RS0002 and RS0004, then the date of admission must be on or after January 1, 2023, and the bill category must be \"OU\".\n",
      "\n",
      "In layman terms, VCA441 appears to be an error code that indicates a discrepancy in how day rehabilitation centers submit their claims. Specifically, it seems that there's an issue with the dates of discharge/admission not matching the corresponding charge codes (RS0001/RS0002) and bill categories (\"DH\"/\"OU\").\n",
      "\n",
      "In this case, I would recommend reviewing the claim submission to ensure that the dates and codes align according to the business rules outlined. If the issue persists, further investigation into the claim data and billing practices may be necessary to resolve the discrepancy.Context:\n",
      "| S/N | Field Name | Business Rule | Error Code | Error Description | Associated Policy | For Claim/Non-Claim | Tag |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "| 416 | Hospital Code<br>Charge Code<br>Date of Discharge<br>Date of Admission | If Hospital Code = (W1-W6, R00-RZZ) and charge code = RS0001 or RP0001, then Date of Discharge must be before 1 Jan 2023 and Bill Category must be DH.<br><br>If Hospital Code =  (W1-W6, R00-RZZ) and charge code = RS0002 - 4, then Date of Admission must be on or after 1 Jan 2023 and Bill Category must be OU. | VCA441 | For day rehabilitation centres, RS0001/RP0001 should only be submitted with Bill Category DH and Date of Discharge before 1 Jan 2023. RS0002 to RS0004 should only be submitted with Bill Category OU and Date of Admission on or after 1 Jan 2023. | SR-MediClaim-202207-0017(MediClaim - Changes to MSV DRC Rehab Limit) | All | DRC | null |"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "\n",
    "prompt_template= PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are an assistant for question-answering tasks. Often, the context might be in code, descriptions or tables, do your best to format it nicely and then analyse them.\n",
    "    You can try to identify the specific things mentioned in the question and work from there.\n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    If you dont know, also mention what you could discern specifically from the context as well as what you think you might need to answer the given question.\n",
    "    \n",
    "    \n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question}\n",
    "    Context: {context}\n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\",\"context\"],\n",
    ")\n",
    "\n",
    "# After formatting the variable \"context\" given to you nicely into a human readable format, return it at the end in the format \"Context: (formatted context)\". \n",
    "#     If no \"context\" variable is given, dont need to add anything.\n",
    "\n",
    "# Prompt template for formatting context\n",
    "context_formatting_template = PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are an assistant for formatting tasks. Do not alter the content or add in any new content.\n",
    "    You are only in charge of reformatting the context given to you. \n",
    "    The context is parsed from a pdf and might be part of code, descriptions or tables. Do your best to format it nicely into markdown.\n",
    "    The context could also be in the form of a JSON, format it into a table if suitable.\n",
    "    Format the given \"context\" variable and return it in the format \"Context: (formatted context)\". \n",
    "    If no \"context\" variable is given, just return an empty string.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Context: {context}\n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"context\"],\n",
    ")\n",
    "\n",
    "# Initialize the LLM\n",
    "local_llm = \"llama3\"\n",
    "llm_main = ChatOllama(model=local_llm, temperature=0, callbacks=callback_manager)\n",
    "llm_formatter = ChatOllama(model=local_llm, temperature=0, callbacks=callback_manager)\n",
    "\n",
    "def format_history(msg, history):\n",
    "    chat_history = [{\"role\": \"system\", \"content\": \"hi\"}]\n",
    "    for query, response in history:\n",
    "        chat_history.append({\"role\": \"user\", \"content\": query})\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "    chat_history.append({\"role\": \"user\", \"content\": msg})\n",
    "    return chat_history\n",
    "\n",
    "def generate_response(message, history, top_k, top_p, temperature, validate=False, check_hallucination=False, check_context=False):\n",
    "    if history is None:\n",
    "        history = []\n",
    "\n",
    "    validation_agent = retrieval_grader\n",
    "    hallucination_agent = hallucination_grader\n",
    "\n",
    "    # # Retrieve and combine documents\n",
    "    # combined_docs = retrieve_and_combine_documents(message, retriever)\n",
    "    # combined_docs_string = \"\\n\\n\".join([str(doc) for doc in combined_docs])\n",
    "\n",
    "    error_code = extract_error_code(message)\n",
    "    combined_docs_string = search_df(loaded_df, search_col , error_code)\n",
    "    print(combined_docs_string)\n",
    "\n",
    "    # Generate the main response\n",
    "    prompt = prompt_template.format(question=message, context=combined_docs_string)\n",
    "    response = llm_main.stream(prompt, top_k = int(top_k), top_p = float(top_p), temperature = float(temperature))\n",
    "    result = \"\"\n",
    "    for partial_answer in response:\n",
    "        result += partial_answer.content\n",
    "        history[-1][1] = result\n",
    "        yield history, gr.update()  # Yield the main response first\n",
    "\n",
    "    # Append the context check result to the main response if requested\n",
    "    if check_context:\n",
    "        context_prompt = context_formatting_template.format(context=combined_docs_string)\n",
    "        context_response = llm_formatter.stream(context_prompt)\n",
    "        result += \"\\n\\nFormatted Context:\\n\"\n",
    "        for partial_context in context_response:\n",
    "            result += partial_context.content\n",
    "            history[-1][1] = result\n",
    "            yield history, gr.update()\n",
    "\n",
    "    # Perform validation check if requested\n",
    "    if validate:\n",
    "        documents = validation_agent.invoke({\"question\": message, \"document\": doc_text})\n",
    "        validation_result = \"Validation Check: Documents validated successfully.\"\n",
    "        result += \"\\n\\n\" + validation_result\n",
    "        history[-1][1] = result\n",
    "        yield history, gr.update()\n",
    "\n",
    "    # Perform hallucination check if requested\n",
    "    if check_hallucination:\n",
    "        hallucination_result = hallucination_agent.invoke({\"documents\": combined_docs, \"generation\": result})\n",
    "        if hallucination_result:\n",
    "            hallucination_result_text = \"\\n\\nHallucination Check:\\n\" + hallucination_result\n",
    "            result += hallucination_result_text\n",
    "            history[-1][1] = result\n",
    "            yield history, gr.update()\n",
    "\n",
    "    history[-1][1] = result\n",
    "    yield history, gr.update()\n",
    "\n",
    "def add_message(history, message):\n",
    "    if message is not None:\n",
    "        history.append([message, None])\n",
    "    return history, gr.update(value=\"\")\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot([], elem_id=\"chatbot\", bubble_full_width=False)\n",
    "    message_input = gr.Textbox(placeholder=\"Enter message...\", show_label=False)\n",
    "    context_checkbox = gr.Checkbox(label=\"Show Context (takes longer)\")\n",
    "    validate_checkbox = gr.Checkbox(label=\"Validate Documents if relevant (takes longer)\")\n",
    "    hallucination_checkbox = gr.Checkbox(label=\"Check if there is hallucination of answer (takes longer)\")\n",
    "    check_answer_checkbox = gr.Checkbox(label=\"Check answer if relevant to question (takes longer)\")\n",
    "    \n",
    "\n",
    "    with gr.Row():\n",
    "        top_k = gr.Slider(0.0,100.0, label=\"top_k\", value=40, info=\"Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)\")\n",
    "        top_p = gr.Slider(0.0,1.0, label=\"top_p\", value=0.9, info=\" Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)\")\n",
    "        temp = gr.Slider(0.0,2.0, label=\"temperature\", value=0.8, info=\"The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8)\")\n",
    "\n",
    "\n",
    "    clear_btn = gr.Button(\"Clear\")\n",
    "    state = gr.State([])  # Initialize state as an empty list to hold the chat history\n",
    "\n",
    "    def gradio_chat_ollama(history, top_k, top_p, temperature, validate, check_hallucination, check_context):\n",
    "        message = history[-1][0] if history else \"\"\n",
    "        generator = generate_response(message, history, top_k, top_p, temperature, validate, check_hallucination, check_context)\n",
    "        for response in generator:\n",
    "            yield response\n",
    "\n",
    "    chat_msg = message_input.submit(add_message, [state, message_input], [state, chatbot]).then(\n",
    "        gradio_chat_ollama, [state,top_k, top_p, temp, validate_checkbox, hallucination_checkbox, context_checkbox], [chatbot, state]\n",
    "    )\n",
    "    clear_btn.click(lambda: [], None, chatbot, queue=False)  # Clear the chat\n",
    "\n",
    "    \n",
    "\n",
    "    demo.queue()\n",
    "    demo.launch(show_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "role, task ,format?\n",
    "\n",
    "any fields to for MI to check first"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
