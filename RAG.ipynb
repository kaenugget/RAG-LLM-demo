{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! MACOSX_DEPLOYMENT_TARGET=14.5 pip install --default-timeout=100 -r requirements2.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm = \"llama3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parse and store pdf data in vector stores (ChromaDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.document_loaders import FireCrawlLoader\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "\n",
    "\n",
    "urls = [\n",
    "    \"https://confluence.ship.gov.sg/pages/viewpage.action?pageId=1050239830&preview=%2F1050239830%2F1252494500%2FHealthcare_7-Feb-2024.pdf\"\n",
    "]\n",
    "\n",
    "docs = [\"pdf/Healthcare_7-Feb-2024.pdf\"]\n",
    "\n",
    "#split documents\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "#split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1024, chunk_overlap=80, length_function=len, is_separator_regex=False\n",
    ")\n",
    "\n",
    "# doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# #Filter out complex metadata and ensure proper document formatting\n",
    "# filtered_docs = []\n",
    "# for doc in doc_splits:\n",
    "#     #ensure the doc is an instance of document and has a 'metadata' attribute\n",
    "#     if isinstance(doc,Document) and has\n",
    "\n",
    "# #add to vectorDB\n",
    "# vectorstore = Chroma.from_documents(\n",
    "#     documents = doc_splits,\n",
    "#     collection_name = \"rag-chroma\",\n",
    "#     embedding = GPT4AllEmbeddings(),\n",
    "# )\n",
    "\n",
    "# retriver = vectorstore.as_retriever()\n",
    "embedding = FastEmbedEmbeddings()\n",
    "\n",
    "vector_store = Chroma(persist_directory=\"db\", embedding_function=embedding)\n",
    "\n",
    "# print(\"creating chain\")\n",
    "retriever = vector_store.as_retriever(\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "\n",
    "embedding = FastEmbedEmbeddings()\n",
    "\n",
    "vector_store = Chroma(persist_directory=\"db\", embedding_function=embedding)\n",
    "\n",
    "# print(\"creating chain\")\n",
    "retriever = vector_store.as_retriever(\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018669843673706055,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Fetching 5 files",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c79e8a501246f3a4dc00bd77bc6634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever created successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "\n",
    "# Step 1: Read the contents of documents.txt\n",
    "with open('db/documents.txt', 'r') as file:\n",
    "    documents = file.readlines()\n",
    "\n",
    "# Ensure the documents are in the right format (list of strings)\n",
    "documents = [doc.strip() for doc in documents]\n",
    "\n",
    "# Step 2: Initialize the embedding model\n",
    "embedding = FastEmbedEmbeddings()\n",
    "\n",
    "# Step 3: Initialize the Chroma vector store with the embedding function\n",
    "persist_directory = \"db\"\n",
    "if not os.path.exists(persist_directory):\n",
    "    os.makedirs(persist_directory)\n",
    "\n",
    "vector_store = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "\n",
    "# Step 4: Add documents to the vector store\n",
    "for document in documents:\n",
    "    vector_store.add_texts([document])\n",
    "\n",
    "# Step 5: Create the retriever\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "print(\"Retriever created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the retriever\n",
    "query = \"Your query here\"\n",
    "results = retrieve(query)\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine document relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved documents:\n",
      "[Document(page_content=\"add VCA034 - INVALID REASON CODE to the claim level errors of 'the claim' ;\"), Document(page_content=\"add VCA032 - INVALID SMC NUMBER to the claim level errors of 'the claim' ;\"), Document(page_content='VCA009 - INVALID'), Document(page_content='VCA009 - INVALID'), Document(page_content='about:blank 373/495'), Document(page_content='about:blank 353/495'), Document(page_content='about:blank 463/495'), Document(page_content='about:blank 310/495')]\n",
      "page_content=\"add VCA034 - INVALID REASON CODE to the claim level errors of 'the claim' ;\"\n",
      "page_content=\"add VCA032 - INVALID SMC NUMBER to the claim level errors of 'the claim' ;\"\n",
      "page_content='VCA009 - INVALID'\n",
      "page_content='VCA009 - INVALID'\n",
      "page_content='about:blank 373/495'\n",
      "page_content='about:blank 353/495'\n",
      "page_content='about:blank 463/495'\n",
      "page_content='about:blank 310/495'\n",
      "Document text:\n",
      "page_content=\"add VCA032 - INVALID SMC NUMBER to the claim level errors of 'the claim' ;\"\n",
      "Grader result:\n",
      "{'score': 'no'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "#LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature = 0)\n",
    "\n",
    "prompt= PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are a grader assessing relevance of a retrieved document to a user question. \n",
    "    If the document contains keywords related to the user question, grade it as relevant.\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate wherther the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the retrieved document: \\n\\n {document}\\n\\n\n",
    "    Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\",\"document\"],\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "question = \"what the reason behind error code VCA373 and write a playbook to investigate an error\"\n",
    "error_code = \"VCA373\"\n",
    "docs = retriever.invoke(question) + retriever.invoke(error_code)\n",
    "\n",
    "# Debug print statements\n",
    "print(\"Retrieved documents:\")\n",
    "print(docs)\n",
    "\n",
    "for ele in docs:\n",
    "    print(ele)\n",
    "\n",
    "# Ensure we have the correct index and document content\n",
    "if len(docs) > 1:\n",
    "    doc_text = docs[1]\n",
    "    print(\"Document text:\")\n",
    "    print(doc_text)\n",
    "\n",
    "    # Invoke the grader\n",
    "    result = retrieval_grader.invoke({\"question\": question, \"document\": doc_text})\n",
    "    print(\"Grader result:\")\n",
    "    print(result)\n",
    "else:\n",
    "    print(\"Insufficient documents retrieved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error_code': 'CHC016'}\n",
      "Extracted Error Code: CHC016\n",
      "Retrieved documents:\n",
      "combined doc 0: TL0001 - Dental\n",
      "combined doc 1: TL0001 - Dental\n",
      "combined doc 2: TL0001 - Dental\n",
      "combined doc 3: TL0001 - Dental\n",
      "combined doc 4: invalid for your hospital code. For further\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "#LLM\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature = 0)\n",
    "\n",
    "prompt= PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are an indentifier.\n",
    "    Identify the error code from the given question / sentence. An example could be VCA373 . Error codes usually start with a few capital alphabets followed by a few numbers\n",
    "    It does not need to be a stringent test. The goal is to identify only the error code and return that. \\n\n",
    "    Return the error code if it is present, else return an empty string ''. \\n\n",
    "    Provide the error code as a JSON with a single key 'error_code' and no preamble or explanation.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the sentence: \\n\\n {question}\\n\\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "error_identifier = prompt | llm | JsonOutputParser()\n",
    "\n",
    "\n",
    "def extract_error_code(question):\n",
    "    error_code_JSON = error_identifier.invoke({\"question\": question})\n",
    "    print(error_code_JSON)\n",
    "    return error_code_JSON[\"error_code\"]\n",
    "\n",
    "### Document Retrieval and Combination\n",
    "def retrieve_and_combine_documents(question, retriever):\n",
    "    error_code = extract_error_code(question)\n",
    "    combined_docs = []\n",
    "\n",
    "    if error_code:\n",
    "        print(f\"Extracted Error Code: {error_code}\")\n",
    "        general_docs = retriever.get_relevant_documents(question)\n",
    "        error_code_docs = retriever.get_relevant_documents(error_code)\n",
    "\n",
    "        # Check for exact match of the error code and prioritize it\n",
    "        exact_match_docs = [doc for doc in error_code_docs if error_code in doc.page_content]\n",
    "\n",
    "        # Combine documents prioritizing the exact match\n",
    "        combined_docs = exact_match_docs + [doc for doc in error_code_docs if doc not in exact_match_docs] + [doc for doc in general_docs if doc not in error_code_docs]\n",
    "\n",
    "    else:\n",
    "        print(\"No Error Code found in the question.\")\n",
    "        combined_docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "    return combined_docs[:5]\n",
    "\n",
    "# Usage Example\n",
    "question = \"What is the reason behind error code CHC016?\"\n",
    "combined_docs = retrieve_and_combine_documents(question, retriever)\n",
    "\n",
    "# Debug print statements\n",
    "print(\"Retrieved documents:\")\n",
    "for i, doc in enumerate(combined_docs):\n",
    "    print(f\"combined doc {i}: {doc.page_content}\")\n",
    "\n",
    "# print(question)\n",
    "# print(combined_docs)\n",
    "# generation = rag_chain.invoke({\"context\": combined_docs, \"question\": question})\n",
    "# print(\"Generated Response:\")\n",
    "# print(generation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_error_code(question):\n",
    "    error_code_JSON = error_identifier.invoke({\"question\": question})\n",
    "    return error_code_JSON[\"error_code\"]\n",
    "\n",
    "### Document Retrieval and Combination\n",
    "def retrieve_and_combine_documents(question, retriever):\n",
    "    error_code = extract_error_code(question)\n",
    "    combined_docs = []\n",
    "\n",
    "    if error_code:\n",
    "        print(f\"Extracted Error Code: {error_code}\")\n",
    "\n",
    "        with open('db/documents.txt', 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        matched_snippets = set()\n",
    "        skip_lines = 0\n",
    "        skip = False\n",
    "        for i, line in enumerate(lines):\n",
    "\n",
    "            skip_lines -= 1\n",
    "\n",
    "            if error_code in line and not skip:\n",
    "                start = max(0, i - 10)\n",
    "                end = min(len(lines), i + 11)\n",
    "                snippet = ''.join(lines[start:end])\n",
    "                matched_snippets.add(snippet)\n",
    "\n",
    "                skip_lines = 10\n",
    "                skip = True\n",
    "\n",
    "                if len(matched_snippets) >= 5:\n",
    "                    break\n",
    "\n",
    "            if skip_lines <= 0:\n",
    "                skip = False\n",
    "                skip = 0\n",
    "\n",
    "\n",
    "\n",
    "        combined_docs = list(matched_snippets)[:5]\n",
    "        print(\"Exact match found in documents.txt\")\n",
    "        print(\"-------------------------------\")\n",
    "        for idx, doc in enumerate(combined_docs):\n",
    "            print(f\"combined_doc {idx}: {doc}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No Error Code found in the question.\")\n",
    "        # Retrieve documents only for the general question if no error code is found\n",
    "        combined_docs = retriever(question)\n",
    "\n",
    "    return combined_docs\n",
    "\n",
    "# # Usage Example\n",
    "# question = \"What is the reason behind error code CHC016?\"\n",
    "# combined_docs = retrieve_and_combine_documents(question, retriever)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Error Code: CHC016\n",
      "Exact match found in documents.txt\n",
      "-------------------------------\n",
      "combined_doc 0: about:blank 94/495\n",
      "Rule: Common-ClaimLimitReached\n",
      "Preconditions:\n",
      "definitions\n",
      "set 'fully utilised limit' to a limit in all limits for 'financing scheme to process' in 'the claim'\n",
      "where the limit has reached for this limit ;\n",
      "Definition:\n",
      "Limit reached scheme Add error code Add scheme name for\n",
      "external remark\n",
      "1 MSHL-Brachytherapy - Brachytherapy\n",
      "CHC016 - MediShield Life\n",
      "claim limit has been\n",
      "reached.\n",
      "MSHL-Brachytherapy\n",
      "2 MSHL-Chemo - Chemotherapy\n",
      "CHC016 - MediShield Life\n",
      "claim limit has been\n",
      "reached.\n",
      "MSHL-Chemo\n",
      "3 MSHL-Chemotherapy Services - MSHL Chemotherapy Services\n",
      "CHC016 - MediShield Life\n",
      "\n",
      "combined_doc 1: CHC009 - MediShield / MediShield Life claimable amount\n",
      "computed is less than or equal to the deductible: there is\n",
      "no payout from MediShield / MediShield Life\n",
      "APPROVE MEDISHIELD_LIFE\n",
      "7 CHC011 - Pregnancy / Childbirth is not covered under\n",
      "MediShield / MediShield Life REJECT MEDISHIELD_LIFE\n",
      "8 CHC013 - Sub-fertility & assisted conception procedures\n",
      "are not covered under MediShield / MediShield Life REJECT MEDISHIELD_LIFE\n",
      "9 CHC015 - THE DESCRIPTION FOR DIAGNOSIS\n",
      "CONTRADICTS WITH THE DESCRIPTION FOR OPERATION REJECT\n",
      "10 CHC016 - MediShield Life claim limit has been reached. APPROVE MEDISHIELD_LIFE\n",
      "11 CHC020 - Treatment for drug addiction/alcoholism is not\n",
      "covered under MediShield / MediShield Life REJECT MEDISHIELD_LIFE\n",
      "12 CHC021 - Cosmetic surgery is not covered under\n",
      "MediShield / MediShield Life REJECT MEDISHIELD_LIFE\n",
      "13\n",
      "CHC022 - Treatment of self-inflicted injuries or injuries\n",
      "resulting from attempted suicide is not covered under\n",
      "MediShield / MediShield Life\n",
      "REJECT MEDISHIELD_LIFE\n",
      "14 CHC023 - CANCELLATION TO A PREVIOUSLY REJECTED\n",
      "\n",
      "combined_doc 2: claim limit has been\n",
      "reached.\n",
      "MSHL-Chemo\n",
      "3 MSHL-Chemotherapy Services - MSHL Chemotherapy Services\n",
      "CHC016 - MediShield Life\n",
      "claim limit has been\n",
      "reached.\n",
      "MSHL-Chemotherapy\n",
      "Services\n",
      "4 MSHL-Dialysis - Renal dialysis treatment\n",
      "CHC016 - MediShield Life\n",
      "claim limit has been\n",
      "reached.\n",
      "MSHL-Dialysis\n",
      "5 MSHL-EPREX - Erythropoietin for Chronic Kidney Failure\n",
      "CHC016 - MediShield Life\n",
      "claim limit has been\n",
      "reached.\n",
      "MSHL-EPREX\n",
      "6 MSHL-Hemi-Body RT - Hemi-Body Radiotherapy\n",
      "CHC016 - MediShield Life\n",
      "\n",
      "combined_doc 3: where the limit has reached for this limit ;\n",
      "Definition:\n",
      "Limit reached scheme Add error code Add scheme name for\n",
      "external remark\n",
      "1 MSHL-Brachytherapy - Brachytherapy\n",
      "CHC016 - MediShield Life\n",
      "claim limit has been\n",
      "reached.\n",
      "MSHL-Brachytherapy\n",
      "2 MSHL-Chemo - Chemotherapy\n",
      "CHC016 - MediShield Life\n",
      "claim limit has been\n",
      "reached.\n",
      "MSHL-Chemo\n",
      "3 MSHL-Chemotherapy Services - MSHL Chemotherapy Services\n",
      "CHC016 - MediShield Life\n",
      "claim limit has been\n",
      "reached.\n",
      "MSHL-Chemotherapy\n",
      "Services\n",
      "4 MSHL-Dialysis - Renal dialysis treatment\n",
      "\n",
      "combined_doc 4: CHC016 - MediShield Life\n",
      "claim limit has been\n",
      "reached.\n",
      "MSHL-Brachytherapy\n",
      "2 MSHL-Chemo - Chemotherapy\n",
      "CHC016 - MediShield Life\n",
      "claim limit has been\n",
      "reached.\n",
      "MSHL-Chemo\n",
      "3 MSHL-Chemotherapy Services - MSHL Chemotherapy Services\n",
      "CHC016 - MediShield Life\n",
      "claim limit has been\n",
      "reached.\n",
      "MSHL-Chemotherapy\n",
      "Services\n",
      "4 MSHL-Dialysis - Renal dialysis treatment\n",
      "CHC016 - MediShield Life\n",
      "claim limit has been\n",
      "reached.\n",
      "MSHL-Dialysis\n",
      "5 MSHL-EPREX - Erythropoietin for Chronic Kidney Failure\n",
      "\n",
      "Context: The context is a list of rules and definitions related to MediShield Life, a health insurance scheme. It appears to be a set of error codes and descriptions, with each code having a specific reason or condition that triggers the error.\n",
      "\n",
      "The specific error code mentioned in the question is CHC016 - MediShield Life claim limit has been reached.Context: The context is a list of rules and definitions related to MediShield Life, a health insurance scheme. It appears to be a set of error codes and descriptions, with each code having a specific reason or condition that triggers the error.\n",
      "\n",
      "The specific error code mentioned in the question is CHC016 - MediShield Life claim limit has been reached.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "prompt= PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are an assistant for question-answering tasks. Often, the context might be in code or descriptions, do your best to format it nicely and then analyse them.\n",
    "    You can try to identify the specific things mentioned in the question and work from there.\n",
    "    After formatting it nicely into a human readable format, return it at the end in the format \"Context: (your formatted context)\"\n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    If you dont know, also mention what you could discern specifically from the context as well as what you think you might need to answer the given question.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question}\n",
    "    Context: {context}\n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\",\"context\"],\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=local_llm, temperature=0, callbacks=callback_manager)\n",
    "\n",
    "#chain\n",
    "\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "#run\n",
    "question = \"What is the reason behind error CHC016\"\n",
    "combined_docs = retrieve_and_combine_documents(question, retriever)\n",
    "# Extract text from combined documents\n",
    "# context = \"\\n\\n\".join([doc for doc in combined_docs])\n",
    "\n",
    "generation = rag_chain.invoke({\"context\": combined_docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for answer for hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'no'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt= PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are a grader assessing whether an answer is grounded in / supported by a set of facts.\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the answer is grounded in / supported by a set of facts.\n",
    "    Provide the binary score as a JSON with a single ey 'score' and no preamble or explanation.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here are the facts: \n",
    "    \\n-------\\n \n",
    "    {documents}\n",
    "    \\n-------\\n\n",
    "    Here is the answer: {generation} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"generation\",\"document\"],\n",
    ")\n",
    "\n",
    "hallucination_grader = prompt | llm | JsonOutputParser()\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt= PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are a grader assessing whether an answer is addressing the question properly.\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the answer addresses the question well.\n",
    "    Provide the binary score as a JSON with a single ey 'score' and no preamble or explanation.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question}\n",
    "    Here is the answer: {generation} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"generation\",\"question\"],\n",
    ")\n",
    "\n",
    "answer_grader = prompt | llm | JsonOutputParser()\n",
    "answer_grader.invoke({\"question\": question, \"generation\": generation})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lang graph - Setup states & nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "import time\n",
    "\n",
    "###state\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]\n",
    "    error_code: str\n",
    "\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "###Nodes\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    error_code_JSON = error_identifier.invoke({\"question\": question})\n",
    "    error_code = error_code_JSON[\"error_code\"]\n",
    "\n",
    "    if error_code != '':\n",
    "        print(f\"Extracted Error Code: {error_code}\")\n",
    "        # # Retrieve documents for the general question\n",
    "        # general_docs = retriever.invoke(question)\n",
    "        \n",
    "        # # Retrieve documents specifically for the error code\n",
    "        # error_code_docs = retriever.invoke(error_code)\n",
    "\n",
    "        # exact_match_docs1 = [doc for doc in general_docs if error_code in doc.page_content]\n",
    "        # exact_match_docs2 = [doc for doc in error_code_docs if error_code in doc.page_content]\n",
    "        # # Combine the results, ensuring error code documents are prioritized or marked\n",
    "        # # combined_docs = error_code_docs + [doc for doc in general_docs if doc not in error_code_docs]\n",
    "\n",
    "        # combined_docs = exact_match_docs1 + [doc for doc in general_docs if doc not in exact_match_docs1]\n",
    "\n",
    "        # print(f\"combined_docs: {combined_docs}\")\n",
    "        # print(\"-------------------------------\")\n",
    "        # print(f\"exac 1: {exact_match_docs1}\")\n",
    "        # print(\"-------------------------------\")\n",
    "        # print(f\"gen docs: {general_docs}\")\n",
    "        # print(\"-------------------------------\")\n",
    "        # print(f\"exac 1: {exact_match_docs2}\")\n",
    "        # print(\"-------------------------------\")\n",
    "        # print(f\"errcodedocs: {error_code_docs}\")\n",
    "\n",
    "\n",
    "        with open('db/documents.txt', 'r') as file:\n",
    "                lines = file.readlines()\n",
    "\n",
    "        matched_lines = []\n",
    "        for i, line in enumerate(lines):\n",
    "            if error_code in line:\n",
    "                start = max(0, i - 20)\n",
    "                end = min(len(lines), i + 21)\n",
    "                matched_lines.extend(lines[start:end])\n",
    "                \n",
    "        if matched_lines:\n",
    "            combined_docs = matched_lines\n",
    "            print(\"Exact match found in documents.txt\")\n",
    "            print(\"-------------------------------\")\n",
    "            print(f\"combined_docs: {combined_docs}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"No Error Code found in the question.\")\n",
    "        # Retrieve documents only for the general question if no error code is found\n",
    "        combined_docs = retriever.invoke(question)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Retrieve time: {elapsed_time:.2f} seconds\")\n",
    "    state[\"retrieve_time\"] = elapsed_time\n",
    "    return {\"documents\": combined_docs, \"question\": question, \"error_code\": error_code}\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "    If any document is not relevant, we will ... (TBD)\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Filtered out irrelevant documents\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    \n",
    "    question = state[\"question\"]\n",
    "    documents = state['documents']\n",
    "\n",
    "    #score each doc\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        print(f\"doc: {d}\")\n",
    "        score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "        grade = score['score']\n",
    "        #document relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        \n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            #then do not include the document in filtered_docs\n",
    "            continue\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Grade documents time: {elapsed_time:.2f} seconds\")\n",
    "    state[\"grade_documents_time\"] = elapsed_time\n",
    "    return {\"documents\": filtered_docs, \"question\": question}\n",
    "\n",
    " \n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(\"---GENERATING---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    #RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Generate time: {elapsed_time:.2f} seconds\")\n",
    "    state[\"generate_time\"] = elapsed_time\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(\"---CHECK HALLUCINATION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
    "    grade = score['score']\n",
    "\n",
    "    #check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"--DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        #check question-answering\n",
    "        print(\"---GRADE GENERATION VS QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score['score']\n",
    "        if grade == \"yes\":\n",
    "            print(\"--DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"Grade generation time: {elapsed_time:.2f} seconds\")\n",
    "            state[\"grade_generation_time\"] = elapsed_time\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"--DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"Grade generation time: {elapsed_time:.2f} seconds\")\n",
    "            state[\"grade_generation_time\"] = elapsed_time\n",
    "            return \"not useful\"\n",
    "\n",
    "    else:\n",
    "        print(\"--DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Grade generation time: {elapsed_time:.2f} seconds\")\n",
    "        state[\"grade_generation_time\"] = elapsed_time\n",
    "        return \"not supported\"\n",
    "\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "#define nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "# workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generate\", generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build graph\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "# workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_edge(\"retrieve\", \"generate\")\n",
    "# workflow.add_edge(\"grade_documents\", \"generate\")\n",
    "\n",
    "# workflow.add_conditional_edges(\n",
    "#     \"generate\",\n",
    "#     grade_generation_v_documents_and_question,\n",
    "#     {\n",
    "#         \"not supported\": \"generate\",\n",
    "#         \"useful\": END,\n",
    "#         # \"not useful\"\n",
    "#     },\n",
    "# )\n",
    "workflow.add_edge(\"generate\", END)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile\n",
    "app = workflow.compile()\n",
    "\n",
    "#Test\n",
    "from pprint import pprint\n",
    "inputs = {\"question\": \"What is the reason behind error code VCA373. Come up with a play book to invstigate this\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "\n",
    "print(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import io\n",
    "import contextlib\n",
    "import sys\n",
    "from typing import Any, Dict\n",
    "import time\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "### Gradio Chat Interface Integration\n",
    "\n",
    "### ChatInterface function with streaming for generation output\n",
    "def chat_predict(message, history, state):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        inputs = {\"question\": message}\n",
    "        full_output = \"\"\n",
    "        state = {\"generation\": \"\"}\n",
    "\n",
    "        for output in app.stream(inputs):\n",
    "            for key, value in output.items():\n",
    "                if key == \"generation\":\n",
    "                    full_output += value['generation'] + \"\\n\"\n",
    "                    state[\"generation\"] += value['generation'] + \"\\n\"\n",
    "                    current_time = time.time() - start_time\n",
    "                    yield full_output + f\"Running time: {current_time:.2f} seconds\\n\"\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        yield full_output + f\"Total time: {total_time:.2f} seconds\\n + {state['generation']}\"\n",
    "    except Exception as e:\n",
    "        yield str(e)\n",
    "\n",
    "### Combine the interfaces\n",
    "chat_interface = gr.ChatInterface(\n",
    "    fn=chat_predict,\n",
    "    title=\"Ask Question\",\n",
    "    # inputs=[\"text\", \"state\"],\n",
    "    # outputs=\"text\",\n",
    "    # state=\"state\"\n",
    ")\n",
    "\n",
    "### Launch the Gradio app\n",
    "chat_interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context generator\n",
    "\n",
    "# Prompt template for formatting context\n",
    "context_formatting_template = PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are an assistant for formatting tasks. Do not alter the content or add in any new content.\n",
    "    You are only in charge of reformatting the context given to you. \n",
    "    The context is parsed from a pdf and might be part of code, descriptions or tables. Do your best to format it nicely into markdown.\n",
    "    Format the given \"context\" variable and return it in the format \"Context: (formatted context)\". \n",
    "    If no \"context\" variable is given, just return an empty string.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Context: {context}\n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"context\"],\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=local_llm, temperature=0, callbacks=callback_manager)\n",
    "context_formatter = prompt | llm | StrOutputParser()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7941\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7941/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Error Code: VCA373\n",
      "Exact match found in documents.txt\n",
      "-------------------------------\n",
      "combined_doc 0: Project: Healthcare\n",
      "Status: new\n",
      "Tags: []\n",
      "Folder Name: [Healthcare]\\common\\screening\\claimitemlevel\n",
      "Rule: COMMON-PredeliveryAppealHRNExemption\n",
      "Definition:\n",
      "if\n",
      "'the current claim item' contains scheme MSV-Predelivery - Pre-delivery [MSV]\n",
      "and it is not true that the appeal outcome of 'the claim' is hrn exempted\n",
      "then\n",
      "add VCA373 - MOH CLAIM EXEMPTION REQUIRED to the claim level errors of 'the claim' ;\n",
      "Documentation:\n",
      "This rule checks that if the claim is MSV-predelivery and the appeal outcome is not hrn exempted error VCA373 will be added to the claim.\n",
      "Properties:\n",
      "Active: true\n",
      "Categories: []\n",
      "Created By: kueh_thian_phin@tech.gov.sg\n",
      "Created On: Wed Jun 02 16:33:30 GMT+08:00 2021\n",
      "Folder: claimitemlevel\n",
      "Group: Healthcare\n",
      "Last Changed By: deepak.agrawal@cognizant.com\n",
      "\n",
      "Based on the provided context, error VCA373 is related to a rule in the Healthcare project. The rule is called \"COMMON-PredeliveryAppealHRNExemption\" and it checks if a claim item contains a specific scheme (MSV-Predelivery) and the appeal outcome of the claim is not HRN exempted.\n",
      "\n",
      "If these conditions are met, the rule adds error VCA373 (\"MOH CLAIM EXEMPTION REQUIRED\") to the claim level errors. Therefore, error VCA373 is likely an error code or message that indicates a claim exemption requirement for the Ministry of Health (MOH).Context:\n",
      "Project: **Healthcare**\n",
      "Status: **new**\n",
      "Tags: []\n",
      "Folder Name: `[Healthcare]\\common\\screening\\claimitemlevel`\n",
      "Rule: **COMMON-PredeliveryAppealHRNExemption**\n",
      "\n",
      "Definition:\n",
      "```markdown\n",
      "if 'the current claim item' contains scheme MSV-Predelivery - Pre-delivery [MSV]\n",
      "and it is not true that the appeal outcome of 'the claim' is hrn exempted\n",
      "then\n",
      "add VCA373 - MOH CLAIM EXEMPTION REQUIRED to the claim level errors of 'the claim'\n",
      "```\n",
      "Documentation:\n",
      "**This rule checks that if the claim is MSV-predelivery and the appeal outcome is not hrn exempted, error VCA373 will be added to the claim.**\n",
      "\n",
      "Properties:\n",
      "* **Active**: `true`\n",
      "* **Categories**: []\n",
      "* **Created By**: kueh_thian_phin@tech.gov.sg\n",
      "* **Created On**: Wed Jun"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "\n",
    "prompt_template= PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are an assistant for question-answering tasks. Often, the context might be in code, descriptions or tables, do your best to format it nicely and then analyse them.\n",
    "    You can try to identify the specific things mentioned in the question and work from there.\n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    If you dont know, also mention what you could discern specifically from the context as well as what you think you might need to answer the given question.\n",
    "    \n",
    "    \n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question}\n",
    "    Context: {context}\n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\",\"context\"],\n",
    ")\n",
    "\n",
    "# After formatting the variable \"context\" given to you nicely into a human readable format, return it at the end in the format \"Context: (formatted context)\". \n",
    "#     If no \"context\" variable is given, dont need to add anything.\n",
    "\n",
    "# Prompt template for formatting context\n",
    "context_formatting_template = PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are an assistant for formatting tasks. Do not alter the content or add in any new content.\n",
    "    You are only in charge of reformatting the context given to you. \n",
    "    The context is parsed from a pdf and might be part of code, descriptions or tables. Do your best to format it nicely into markdown.\n",
    "    Format the given \"context\" variable and return it in the format \"Context: (formatted context)\". \n",
    "    If no \"context\" variable is given, just return an empty string.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Context: {context}\n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"context\"],\n",
    ")\n",
    "\n",
    "# Initialize the LLM\n",
    "local_llm = \"llama3\"  # Assuming llama3 is already defined\n",
    "llm = ChatOllama(model=local_llm, temperature=0, callbacks=callback_manager)\n",
    "llm_formatter = ChatOllama(model=local_llm, temperature=0, callbacks=callback_manager)\n",
    "\n",
    "def format_history(msg, history):\n",
    "    chat_history = [{\"role\": \"system\", \"content\": \"hi\"}]\n",
    "    for query, response in history:\n",
    "        chat_history.append({\"role\": \"user\", \"content\": query})\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "    chat_history.append({\"role\": \"user\", \"content\": msg})\n",
    "    return chat_history\n",
    "\n",
    "def generate_response(message, history, validate=False, check_hallucination=False, check_context=False):\n",
    "    if history is None:\n",
    "        history = []\n",
    "\n",
    "    validation_agent = retrieval_grader\n",
    "    hallucination_agent = hallucination_grader\n",
    "\n",
    "    # Retrieve and combine documents\n",
    "    combined_docs = retrieve_and_combine_documents(message, retriever)\n",
    "    combined_docs_string = \"\\n\\n\".join([str(doc) for doc in combined_docs])\n",
    "\n",
    "    # Generate the response\n",
    "    prompt = prompt_template.format(question=message, context=combined_docs_string)\n",
    "    response = llm.stream(prompt)\n",
    "    result = \"\"\n",
    "    for partial_answer in response:\n",
    "        result += partial_answer.content\n",
    "        history[-1][1] = result\n",
    "        yield history, gr.update()  # Yield the main response first\n",
    "\n",
    "    # Perform validation check if requested\n",
    "    if validate:\n",
    "        documents = validation_agent.invoke({\"question\": message, \"document\": doc_text})\n",
    "        validation_result = \"Validation Check: Documents validated successfully.\"\n",
    "        history.append([\"Validation Check\", validation_result])\n",
    "        yield history, gr.update(), validation_result\n",
    "\n",
    "    # Perform context check if requested\n",
    "    if check_context:\n",
    "        context_prompt = context_formatting_template.format(context=combined_docs_string)\n",
    "        context_response = llm_formatter.stream(context_prompt)\n",
    "        context_result = \"\"\n",
    "        for partial_context in context_response:\n",
    "            context_result += partial_context.content\n",
    "\n",
    "        # print(1)\n",
    "        # history.append([\"Context Check\", context_result])\n",
    "            history[-1][1] = result\n",
    "            yield history, gr.update()\n",
    "\n",
    "    # Perform hallucination check if requested\n",
    "    if check_hallucination:\n",
    "        hallucination_result = hallucination_agent.invoke({\"documents\": combined_docs, \"generation\": result})\n",
    "        if hallucination_result:\n",
    "            hallucination_result_text = \"\\n\\nHallucination Check:\\n\" + hallucination_result\n",
    "            history.append([\"Hallucination Check\", hallucination_result_text])\n",
    "            yield history, gr.update(), hallucination_result_text\n",
    "\n",
    "    history[-1][1] = result\n",
    "    print(2)\n",
    "    yield history, gr.update()\n",
    "\n",
    "def add_message(history, message):\n",
    "    if message is not None:\n",
    "        history.append([message, None])\n",
    "    return history, gr.update(value=\"\")\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot([], elem_id=\"chatbot\", bubble_full_width=False)\n",
    "    message_input = gr.Textbox(placeholder=\"Enter message...\", show_label=False)\n",
    "    context_checkbox = gr.Checkbox(label=\"Show Context\")\n",
    "    validate_checkbox = gr.Checkbox(label=\"Validate Documents\")\n",
    "    hallucination_checkbox = gr.Checkbox(label=\"Check Hallucination\")\n",
    "    clear_btn = gr.Button(\"Clear\")\n",
    "    state = gr.State([])  # Initialize state as an empty list to hold the chat history\n",
    "\n",
    "    def gradio_chat_ollama(history, validate, check_hallucination, check_context):\n",
    "        message = history[-1][0] if history else \"\"\n",
    "        generator = generate_response(message, history, validate, check_hallucination, check_context)\n",
    "        print(\"doneonenenneeoneo\")\n",
    "        for response in generator:\n",
    "            print(\"response\")\n",
    "            yield response\n",
    "\n",
    "    chat_msg = message_input.submit(add_message, [state, message_input], [state, chatbot]).then(\n",
    "        gradio_chat_ollama, [state, validate_checkbox, hallucination_checkbox, context_checkbox], [chatbot, state]\n",
    "    )\n",
    "    clear_btn.click(lambda: [], None, chatbot, queue=False)  # Clear the chat\n",
    "\n",
    "    demo.queue()\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def gradio_chat_ollama(message, history, validate, check_hallucination):\n",
    "#     return chat_ollama(message, history, validate, check_hallucination)\n",
    "\n",
    "# with gr.Blocks() as demo:\n",
    "#     chatbot = gr.Chatbot()\n",
    "#     with gr.Row():\n",
    "#         with gr.Column(scale=7):\n",
    "#             textbox = gr.Textbox(placeholder=\"Example: What is error VCA373?\", container=False)\n",
    "#         with gr.Column(scale=3):\n",
    "#             validate_checkbox = gr.Checkbox(label=\"Validate Documents\")\n",
    "#             hallucination_checkbox = gr.Checkbox(label=\"Check Hallucination\")\n",
    "#     clear_btn = gr.Button(\"Clear\")\n",
    "\n",
    "#     textbox.submit(gradio_chat_ollama, [textbox, chatbot, validate_checkbox, hallucination_checkbox], chatbot)\n",
    "#     clear_btn.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "#     demo.launch()\n",
    "\n",
    "\n",
    "# # Define the Gradio interface\n",
    "# with gr.Blocks() as demo:\n",
    "#     gr.Markdown(\"\"\"<h1><center>Chat with Ollama</center></h1>\"\"\")\n",
    "#     chatbot = gr.Chatbot()\n",
    "#     message = gr.Textbox(placeholder=\"Example: What is error VCA373?\", container=False, scale=7)\n",
    "#     validate_checkbox = gr.Checkbox(label=\"Validate Documents\")\n",
    "#     hallucination_checkbox = gr.Checkbox(label=\"Check Hallucination\")\n",
    "#     clear_btn = gr.Button(\"Clear\")\n",
    "#     state = gr.State()\n",
    "\n",
    "#     def gradio_chat_ollama(message, history, validate, check_hallucination):\n",
    "#         return chat_ollama(message, history, validate, check_hallucination)\n",
    "\n",
    "#     message.submit(gradio_chat_ollama, inputs=[message, state, validate_checkbox, hallucination_checkbox], outputs=[chatbot, state])\n",
    "#     clear_btn.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "#     demo.launch()\n",
    "\n",
    "\n",
    "# # Define the Gradio interface\n",
    "# chatbot = gr.ChatInterface(\n",
    "#     generate_response,\n",
    "#     chatbot=gr.Chatbot(\n",
    "#         avatar_images=[\"user.jpg\", \"chatbot.png\"],\n",
    "#         height=\"64vh\"\n",
    "#     ),\n",
    "#     additional_inputs=[\n",
    "#         gr.Textbox(\n",
    "#             \"Behave as if you are a professional writer.\",\n",
    "#             label=\"System Prompt\"\n",
    "#         )\n",
    "#     ],\n",
    "#     title=\"LLama-3 Chatbot using 'Ollama'\",\n",
    "#     description=\"Feel free to ask any question.\",\n",
    "#     theme=\"soft\",\n",
    "#     submit_btn=\"⬅ Send\",\n",
    "#     retry_btn=\"🔄 Regenerate Response\",\n",
    "#     undo_btn=\"↩ Delete Previous\",\n",
    "#     clear_btn=\"🗑️ Clear Chat\"\n",
    "# )\n",
    "\n",
    "# chatbot.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7931\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7931/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message received in gradio_chat_ollama: What is error VCA373?\n",
      "Extracted Error Code: VCA373\n",
      "Exact match found in documents.txt\n",
      "-------------------------------\n",
      "combined_doc 0: Project: Healthcare\n",
      "Status: new\n",
      "Tags: []\n",
      "Folder Name: [Healthcare]\\common\\screening\\claimitemlevel\n",
      "Rule: COMMON-PredeliveryAppealHRNExemption\n",
      "Definition:\n",
      "if\n",
      "'the current claim item' contains scheme MSV-Predelivery - Pre-delivery [MSV]\n",
      "and it is not true that the appeal outcome of 'the claim' is hrn exempted\n",
      "then\n",
      "add VCA373 - MOH CLAIM EXEMPTION REQUIRED to the claim level errors of 'the claim' ;\n",
      "Documentation:\n",
      "This rule checks that if the claim is MSV-predelivery and the appeal outcome is not hrn exempted error VCA373 will be added to the claim.\n",
      "Properties:\n",
      "Active: true\n",
      "Categories: []\n",
      "Created By: kueh_thian_phin@tech.gov.sg\n",
      "Created On: Wed Jun 02 16:33:30 GMT+08:00 2021\n",
      "Folder: claimitemlevel\n",
      "Group: Healthcare\n",
      "Last Changed By: deepak.agrawal@cognizant.com\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/admin/Library/Python/3.9/lib/python/site-packages/gradio/queueing.py\", line 521, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/Users/admin/Library/Python/3.9/lib/python/site-packages/gradio/route_utils.py\", line 276, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/admin/Library/Python/3.9/lib/python/site-packages/gradio/blocks.py\", line 1935, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/admin/Library/Python/3.9/lib/python/site-packages/gradio/blocks.py\", line 1511, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "  File \"/Users/admin/Library/Python/3.9/lib/python/site-packages/gradio/utils.py\", line 799, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "  File \"/var/folders/mt/wb72h12n3yz8mzp138cssdw40000gn/T/ipykernel_17477/2056331426.py\", line 102, in gradio_chat_ollama\n",
      "    await run_tasks_concurrently(history, combined_docs_string, message)\n",
      "  File \"/var/folders/mt/wb72h12n3yz8mzp138cssdw40000gn/T/ipykernel_17477/2056331426.py\", line 78, in run_tasks_concurrently\n",
      "    format_task = asyncio.create_task(format_context(combined_docs_string, history))\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/tasks.py\", line 361, in create_task\n",
      "    task = loop.create_task(coro)\n",
      "  File \"uvloop/loop.pyx\", line 1435, in uvloop.loop.Loop.create_task\n",
      "TypeError: a coroutine was expected, got <async_generator object format_context at 0x12673e1f0>\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# Prompt template for formatting context\n",
    "context_formatting_template = PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are an assistant for formatting tasks. Do not alter the content or add in any new content.\n",
    "    You are only in charge of reformatting the context given to you. \n",
    "    The context is parsed from a pdf and might be part of code, descriptions or tables. Do your best to format it nicely into markdown.\n",
    "    Format the given \"context\" variable and return it in the format \"Context: (formatted context)\". \n",
    "    If no \"context\" variable is given, just return an empty string.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Context: {context}\n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"context\"],\n",
    ")\n",
    "\n",
    "# Prompt template for answering the question\n",
    "prompt_template= PromptTemplate(\n",
    "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "    You are an assistant for question-answering tasks. Often, the context might be in code or descriptions, do your best to analyse them.\n",
    "    You can try to identify the specific things mentioned in the question and context and work from there.\n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    If you dont know, also mention what you could discern specifically from the context as well as what you think you might need to answer the given question.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question}\n",
    "    Context: {context}\n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\",\"context\"],\n",
    ")\n",
    "\n",
    "# Initialize the LLMs\n",
    "local_llm = \"llama3\"  # Assuming llama3 is already defined\n",
    "llm_formatter = ChatOllama(model=local_llm, temperature=0, callbacks=callback_manager)\n",
    "llm_answer = ChatOllama(model=local_llm, temperature=0, callbacks=callback_manager)\n",
    "\n",
    "def format_history(msg, history):\n",
    "    chat_history = [{\"role\": \"system\", \"content\": \"hi\"}]\n",
    "    for query, response in history:\n",
    "        chat_history.append({\"role\": \"user\", \"content\": query})\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "    chat_history.append({\"role\": \"user\", \"content\": msg})\n",
    "    return chat_history\n",
    "\n",
    "# Wrapper to convert a synchronous generator to an asynchronous generator\n",
    "async def async_stream(sync_gen):\n",
    "    for item in sync_gen:\n",
    "        yield item\n",
    "\n",
    "async def format_context(context, history):\n",
    "    print(f\"[{datetime.now()}] Starting context formatting\")\n",
    "    context_prompt = context_formatting_template.format(context=context)\n",
    "    formatted_context_response = llm_formatter.stream(context_prompt)\n",
    "    formatted_context = \"\"\n",
    "    async for partial_response in async_stream(formatted_context_response):\n",
    "        formatted_context += partial_response.content\n",
    "        history[-1][1] = f\"Formatted Context: {formatted_context}\"\n",
    "        yield history, gr.update()\n",
    "    print(f\"[{datetime.now()}] Finished context formatting\")\n",
    "\n",
    "async def generate_answer(question, context, history):\n",
    "    print(f\"[{datetime.now()}] Starting answer generation\")\n",
    "    prompt = prompt_template.format(question=question, context=context)\n",
    "    response = llm_answer.stream(prompt)\n",
    "    result = \"\"\n",
    "    async for partial_answer in async_stream(response):\n",
    "        result += partial_answer.content\n",
    "        history.append([f\"Generated Answer: {result}\", None])\n",
    "        yield history, gr.update()\n",
    "    print(f\"[{datetime.now()}] Finished answer generation\")\n",
    "\n",
    "async def run_tasks_concurrently(history, combined_docs_string, question):\n",
    "    format_task = asyncio.create_task(format_context(combined_docs_string, history))\n",
    "    answer_task = asyncio.create_task(generate_answer(question, combined_docs_string, history))\n",
    "    await asyncio.gather(format_task, answer_task)\n",
    "\n",
    "def add_message(history, message):\n",
    "    if message is not None:\n",
    "        history.append([message, None])\n",
    "    return history, gr.update(value=\"\"), gr.update(value=\"\")\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot([], elem_id=\"chatbot\", bubble_full_width=False)\n",
    "    message_input = gr.Textbox(placeholder=\"Enter message...\", show_label=False)\n",
    "    validate_checkbox = gr.Checkbox(label=\"Validate Documents\")\n",
    "    hallucination_checkbox = gr.Checkbox(label=\"Check Hallucination\")\n",
    "    clear_btn = gr.Button(\"Clear\")\n",
    "    state = gr.State([])  # Initialize state as an empty list to hold the chat history\n",
    "\n",
    "    async def gradio_chat_ollama(history, validate, check_hallucination):\n",
    "        message = history[-1][0] if history else \"\"\n",
    "        print(f\"Message received in gradio_chat_ollama: {message}\")  # Debugging statement\n",
    "\n",
    "        combined_docs = retrieve_and_combine_documents(message, None)\n",
    "        combined_docs_string = \"\\n\\n\".join([str(doc) for doc in combined_docs])\n",
    "\n",
    "        await run_tasks_concurrently(history, combined_docs_string, message)\n",
    "\n",
    "    chat_msg = message_input.submit(add_message, [state, message_input], [state, chatbot, message_input]).then(\n",
    "        gradio_chat_ollama, [state, validate_checkbox, hallucination_checkbox], [chatbot, state]\n",
    "    )\n",
    "    clear_btn.click(lambda: [], None, chatbot, queue=False)  # Clear the chat\n",
    "\n",
    "    demo.queue()\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
